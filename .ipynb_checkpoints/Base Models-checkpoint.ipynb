{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading the data, into a Data Frame.\n",
    "Data = pd.read_csv('/home/prajwal/Desktop/bank-additional/bank-additional-full.csv',delimiter=';',header=0)\n",
    "\n",
    "#Encoding the data, encoding the string values into numerical values.\n",
    "encode = preprocessing.LabelEncoder()\n",
    "\n",
    "#Selcting the columns of string data type\n",
    "names = Data.select_dtypes(include=['object'])\n",
    "\n",
    "#Function that encodes the string values to numerical values.\n",
    "def enc(data,column):\n",
    "    data[column] = encode.fit_transform(data[column])\n",
    "    return data\n",
    "for column in names:\n",
    "        Data = enc(Data,column)\n",
    "        \n",
    "#Splitting the data into training and testing datasets\n",
    "Data, test = train_test_split(Data, test_size = 0.05)\n",
    "\n",
    "#Initializing two data frames that will be used as training data for the stacked model.\n",
    "stack_X = pd.DataFrame() #The data frame will contain the predictions of the base models.\n",
    "stack_Y = pd.DataFrame() #The data frame will contain the calss labels of the base models.\n",
    "\n",
    "#Initializing two data frames that will be used as training data for the blending model.\n",
    "blend_X = pd.DataFrame() #The data frames will contain the predictions and raw features  of the base models.\n",
    "raw_features_X = pd.DataFrame() #The data frames will contain the raw features  of the data, which will be concatenated with the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used to convert the predictions of the base models into a DataFrame.\n",
    "def build_data_frame(data):\n",
    "    data_frame = pd.DataFrame(data).T\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the parameters for the XGBoost (Gradient Boosting) Algorithm.\n",
    "def param_set():\n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = 0.3\n",
    "    param['gamma'] = 0\n",
    "    param['max_depth'] = 6\n",
    "    param['min_child_weight'] = 1\n",
    "    param['max_delta_step'] = 0\n",
    "    param['subsample'] = 1\n",
    "    param['colsample_bytree'] = 1\n",
    "    param['silent'] = 1\n",
    "    param['seed'] = 0\n",
    "    param['base_score'] = 0.5\n",
    "    param['lambda_bias'] = 1\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function is used to train the base and stacking models. Returns all the models to be used for further computations.\n",
    "def train_models(train_X,train_Y,model):\n",
    "    \n",
    "    param = param_set()\n",
    "    #Trains only the base models.\n",
    "    if(model=='base'):\n",
    "        \n",
    "        #Gradient Boosting\n",
    "        dtrain = xgb.DMatrix(train_X,label=train_Y)\n",
    "        gradient_boosting = xgb.train(param, dtrain)\n",
    "        \n",
    "        #Multi Layer Perceptron\n",
    "        multi_layer_perceptron = Sequential()\n",
    "        multi_layer_perceptron.add(Dense(output_dim = 64, input_dim = 20, init = 'uniform', activation = 'sigmoid'))\n",
    "        multi_layer_perceptron.add(Dense(output_dim = 1, input_dim = 64,activation = 'sigmoid',))\n",
    "        multi_layer_perceptron.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "        multi_layer_perceptron.fit(train_X.as_matrix(), train_Y.as_matrix(), nb_epoch = 5, batch_size = 128)\n",
    "        \n",
    "        #Decision Tree\n",
    "        decision_tree = DecisionTreeClassifier(max_depth = 6)\n",
    "        decision_tree.fit(train_X,train_Y)\n",
    "        \n",
    "        #Random Forest (Deafult=10 Trees)\n",
    "        random_forest = RandomForestClassifier()\n",
    "        random_forest.fit(train_X,train_Y)\n",
    "        \n",
    "        #Scaling the data\n",
    "        train_X = preprocessing.StandardScaler().fit_transform(train_X) \n",
    "        \n",
    "        #Linear Regression\n",
    "        linear_regression = linear_model.LinearRegression()\n",
    "        linear_regression.fit(train_X,train_Y)\n",
    "        \n",
    "        #Logistic Regression (L1)\n",
    "        logistic_regression_L1 = linear_model.LogisticRegression(penalty = 'l1')\n",
    "        logistic_regression_L1.fit(train_X,train_Y)\n",
    "        \n",
    "        #Logistic Regression (L2)\n",
    "        logistic_regression_L2 = linear_model.LogisticRegression(penalty = 'l2')\n",
    "        logistic_regression_L2.fit(train_X,train_Y)\n",
    "        \n",
    "        #Returns a dictionary containing the model names and their respective models.\n",
    "        return {'XGBoost':gradient_boosting,'Multi Layer Perceptron':multi_layer_perceptron,'Decision Tree':decision_tree,\n",
    "           'Random Forest':random_forest,'Linear Regression':linear_regression,'L1':logistic_regression_L1,\n",
    "            'L2':logistic_regression_L2}\n",
    "    \n",
    "    #Trains the stacking model (Gradient Boosting - XGBoost)\n",
    "    elif(model == 'stack'):\n",
    "        \n",
    "        dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "        stack = xgb.train(param, dtrain)\n",
    "        return {'Stack':stack}\n",
    "    \n",
    "    #Trains the blending model (Gradient Boosting - XGBoost)\n",
    "    else:\n",
    "        \n",
    "        dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "        blend = xgb.train(param, dtrain)\n",
    "        return {'Blend':blend}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function calculates area under the curve and predictions on the given data, for the model specified.\n",
    "def cross_validation(model_name,model,cross_val_X,cross_val_Y):\n",
    "    \n",
    "    if(model_name == 'Gradient Boosting' or model_name == 'Linear Regression'):\n",
    "        \n",
    "        predict = model.predict(cross_val_X)\n",
    "        \n",
    "    elif(model_name == 'Multi Layer Perceptron'):\n",
    "        \n",
    "        predict = model.predict_on_batch(cross_val_X)\n",
    "    else:\n",
    "        \n",
    "        predict = model.predict_proba(cross_val_X)[:,1]\n",
    "        \n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    \n",
    "    return[auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialzing the variables that will be used to calculate the area under the curve. (cross Validation Data)\n",
    "metric_linear_regression=list()\n",
    "avg_linear_regeression=0\n",
    "metric_logistic_regression_L2=list()\n",
    "avg_logistic_regression_L2=0\n",
    "metric_logistic_regression_L1=list()\n",
    "avg_logistic_regression_L1=0\n",
    "metric_decision_tree=list()\n",
    "avg_decision_tree=0\n",
    "metric_random_forest=list()\n",
    "avg_random_forest=0\n",
    "metric_multi_layer_perceptron=list()\n",
    "avg_multi_layer_perceptron=0\n",
    "metric_gradient_boosting=list()\n",
    "avg_gradient_boosting=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cross Validation using Stratified K Fold\n",
    "kf = StratifiedKFold(Data['y'], n_folds=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31301/31301 [==============================] - 0s - loss: 0.3033 - acc: 0.8876     \n",
      "Epoch 2/5\n",
      "31301/31301 [==============================] - 0s - loss: 0.2782 - acc: 0.8876     \n",
      "Epoch 3/5\n",
      "31301/31301 [==============================] - 0s - loss: 0.2647 - acc: 0.8893     \n",
      "Epoch 4/5\n",
      "31301/31301 [==============================] - 0s - loss: 0.2577 - acc: 0.8975     \n",
      "Epoch 5/5\n",
      "31301/31301 [==============================] - 0s - loss: 0.2570 - acc: 0.8965     \n",
      "Epoch 1/5\n",
      "31302/31302 [==============================] - 0s - loss: 0.4524 - acc: 0.7722     \n",
      "Epoch 2/5\n",
      "31302/31302 [==============================] - 0s - loss: 0.2798 - acc: 0.8876     \n",
      "Epoch 3/5\n",
      "31302/31302 [==============================] - 0s - loss: 0.2667 - acc: 0.8907     \n",
      "Epoch 4/5\n",
      "31302/31302 [==============================] - 0s - loss: 0.2584 - acc: 0.8983     \n",
      "Epoch 5/5\n",
      "31302/31302 [==============================] - 0s - loss: 0.2541 - acc: 0.9043     \n",
      "Epoch 1/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.4454 - acc: 0.7903     \n",
      "Epoch 2/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.3114 - acc: 0.8876     \n",
      "Epoch 3/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2868 - acc: 0.8876     \n",
      "Epoch 4/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2715 - acc: 0.8876     \n",
      "Epoch 5/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2619 - acc: 0.8903     \n",
      "Epoch 1/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.3676 - acc: 0.8592     \n",
      "Epoch 2/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2975 - acc: 0.8876     \n",
      "Epoch 3/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2756 - acc: 0.8876     \n",
      "Epoch 4/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2669 - acc: 0.8877     \n",
      "Epoch 5/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2616 - acc: 0.8926     \n",
      "Epoch 1/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.3536 - acc: 0.8852     \n",
      "Epoch 2/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.3099 - acc: 0.8876     \n",
      "Epoch 3/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2944 - acc: 0.8876     \n",
      "Epoch 4/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2831 - acc: 0.8879     \n",
      "Epoch 5/5\n",
      "31303/31303 [==============================] - 0s - loss: 0.2734 - acc: 0.8897     \n"
     ]
    }
   ],
   "source": [
    "#Training the base models, and calculating AUC on the cross validation data.\n",
    "for train_index, cross_val_index in kf:\n",
    "    \n",
    "    #Selecting the data (Traing Data & Cross Validation Data)\n",
    "    train, cross_val = Data.iloc[train_index], Data.iloc[cross_val_index]\n",
    "    train_Y=train['y']\n",
    "    train_X=train.drop(['y'],axis=1)\n",
    "    cross_val_Y=cross_val['y']\n",
    "    cross_val_X=cross_val.drop(['y'],axis=1)\n",
    "    \n",
    "    #Training the base models, the resulting model names and models are stored in the variable model in the from of a dictionary.\n",
    "    model=train_models(train_X,train_Y,'base')\n",
    "  \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Cross Validation Data)\n",
    "    [auc,predict_gradient_boosting]=cross_validation('Gradient Boosting',model['XGBoost'],xgb.DMatrix(cross_val_X,label=cross_val_Y),cross_val_Y)\n",
    "    metric_gradient_boosting.append(auc)\n",
    "\n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Cross Validation Data)\n",
    "    predict_mlp=list()\n",
    "    [auc,predict_multi_layer_perceptron]=cross_validation('Multi Layer Perceptron',model['Multi Layer Perceptron'],cross_val_X,cross_val_Y)\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "    #predict_multi_layer_perceptron returns a list of lists containing the predictions, this cannot be converted to a dataframe.\n",
    "    #This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "    for i in predict_multi_layer_perceptron:\n",
    "        predict_mlp.append(float(i))\n",
    "    \n",
    "    #Decision Tree)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_decision_tree]=cross_validation('Decision Tree',model['Decision Tree'],cross_val_X,cross_val_Y)\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_random_forest]=cross_validation('Random Forest',model['Random Forest'],cross_val_X,cross_val_Y)\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    #Scaling the cross validation data.\n",
    "    cross_val_X=preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    \n",
    "    #Linear Regression\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_linear_regression]=cross_validation('Linear Regression',model['Linear Regression'],cross_val_X,cross_val_Y)\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_logistic_regression_L2]=cross_validation('L2',model['L2'],cross_val_X,cross_val_Y)\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L1\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_logistic_regression_L1]=cross_validation('L1',model['L1'],cross_val_X,cross_val_Y)\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    #Building a list that contains all the predictions of the base models.\n",
    "    predict_list=[predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    \n",
    "    #Converting the above list of predictions into a dataframe, which will be used to train the stacking model.\n",
    "    stack_Y=stack_Y.append(cross_val_Y.tolist())\n",
    "    stack_X=stack_X.append(build_data_frame(predict_list))\n",
    "    \n",
    "    #Building a list that contains all the raw features used as cross validation data for the base models.\n",
    "    raw_features_X=raw_features_X.append(cross_val_X.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating the average AUC across all the AUC computed on the cross validation folds.\n",
    "avg_linear_regression=np.mean(metric_linear_regression)\n",
    "avg_logistic_regression_L2=np.mean(metric_logistic_regression_L2)\n",
    "avg_logistic_regression_L1=np.mean(metric_logistic_regression_L1)\n",
    "avg_decision_tree=np.mean(metric_decision_tree)\n",
    "avg_random_forest=np.mean(metric_random_forest)\n",
    "avg_multi_layer_perceptron=np.mean(metric_multi_layer_perceptron)\n",
    "avg_gradient_boosting=np.mean(metric_gradient_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC (Linear Regression)\n",
      " 0.928246410045\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.929779343811\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.929792556106\n",
      " AUC (Decision Tree)\n",
      " 0.925532272518\n",
      " AUC (Random Forest)\n",
      " 0.916621150704\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.873806866997\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.947318229086\n"
     ]
    }
   ],
   "source": [
    "#Printing the AUC for the base models.\n",
    "print (' AUC (Linear Regression)\\n',avg_linear_regression)\n",
    "print (' AUC (Logistic Regression - L2)\\n',avg_logistic_regression_L2)\n",
    "print (' AUC (Logistic Regression - L1)\\n',avg_logistic_regression_L1)\n",
    "print (' AUC (Decision Tree)\\n',avg_decision_tree)\n",
    "print (' AUC (Random Forest)\\n',avg_random_forest)\n",
    "print (' AUC (Multi Layer Perceptron)\\n',avg_multi_layer_perceptron)\n",
    "print (' AUC (Gradient Boosting - XGBoost)\\n',avg_gradient_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6   \\\n",
      "0     1.062258 -0.742890  1.350094 -0.820569  1.955895  0.933055 -0.443356   \n",
      "1    -0.668064  1.492528 -0.295373  0.579502 -0.511275  0.933055 -0.443356   \n",
      "2     0.870000  0.933673 -0.295373 -0.353879 -0.511275  0.933055 -0.443356   \n",
      "3     0.485484 -0.742890 -0.295373 -1.287259  1.955895  0.933055 -0.443356   \n",
      "4     1.735161 -0.742890 -0.295373 -0.820569  1.955895  0.933055 -0.443356   \n",
      "5     0.100968 -1.022318 -0.295373 -0.820569  1.955895  0.933055 -0.443356   \n",
      "6     0.293226 -1.022318  1.350094  1.046193 -0.511275  0.933055 -0.443356   \n",
      "7     1.735161  0.095392 -0.295373  1.046193 -0.511275  0.933055  2.351770   \n",
      "8     0.773871 -1.022318 -0.295373 -0.353879 -0.511275  0.933055 -0.443356   \n",
      "9    -1.437096 -1.022318  1.350094 -0.353879 -0.511275  0.933055 -0.443356   \n",
      "10   -0.475806 -1.022318  1.350094  1.512883 -0.511275  0.933055 -0.443356   \n",
      "11    1.735161  0.374819 -1.940840  1.046193 -0.511275  0.933055 -0.443356   \n",
      "12   -0.475806  1.492528 -1.940840  0.579502  1.955895  0.933055  2.351770   \n",
      "13   -1.340967  2.051383  1.350094 -0.820569 -0.511275  0.933055  2.351770   \n",
      "14    1.542903  0.095392 -0.295373  1.046193 -0.511275  0.933055 -0.443356   \n",
      "15    3.946128  0.374819 -1.940840 -1.753950 -0.511275  0.933055 -0.443356   \n",
      "16    1.735161 -0.742890  1.350094 -1.753950  1.955895 -1.095844 -0.443356   \n",
      "17   -0.956451 -1.022318  1.350094  1.046193  1.955895 -1.095844 -0.443356   \n",
      "18   -1.244838  1.492528  1.350094 -0.353879 -0.511275 -1.095844 -0.443356   \n",
      "19    1.350645 -0.742890 -0.295373  0.579502 -0.511275 -1.095844 -0.443356   \n",
      "20    1.254516  0.654246 -0.295373 -0.820569 -0.511275  0.933055  2.351770   \n",
      "21   -1.052580 -0.742890  1.350094 -0.820569 -0.511275  0.933055 -0.443356   \n",
      "22   -1.148709  0.933673 -0.295373 -0.353879 -0.511275  0.933055  2.351770   \n",
      "23   -0.283548 -1.022318 -0.295373 -0.353879 -0.511275  0.933055 -0.443356   \n",
      "24    0.485484  1.492528 -0.295373 -0.353879 -0.511275 -1.095844 -0.443356   \n",
      "25   -1.052580 -0.742890  1.350094 -0.353879  1.955895 -1.095844 -0.443356   \n",
      "26    0.773871 -1.022318 -0.295373  1.046193 -0.511275  0.933055  2.351770   \n",
      "27    0.197097 -1.022318 -0.295373  1.046193 -0.511275  0.933055 -0.443356   \n",
      "28   -1.052580 -1.022318 -0.295373 -1.287259 -0.511275  0.933055 -0.443356   \n",
      "29    1.542903  0.095392 -1.940840  1.512883 -0.511275  0.933055 -0.443356   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7795 -0.762559 -1.037719  1.341981  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7796 -1.048683 -0.759259 -0.299313 -0.799389 -0.514935 -0.058433  0.913020   \n",
      "7797  1.717184 -1.037719  1.341981  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7798 -0.857934 -1.037719  1.341981 -0.330552 -0.514935  0.955395 -0.457473   \n",
      "7799  0.763437 -1.037719 -0.299313  0.607121 -0.514935  0.955395 -0.457473   \n",
      "7800 -1.716307  1.189955  1.341981 -0.330552 -0.514935 -1.072260 -0.457473   \n",
      "7801 -0.190311  1.468414 -0.299313  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7802 -0.953309  1.468414 -0.299313 -0.330552 -0.514935 -1.072260 -0.457473   \n",
      "7803 -1.239433  1.189955  1.341981 -0.330552 -0.514935  0.955395  2.283513   \n",
      "7804 -0.285685  1.468414 -0.299313  0.607121 -0.514935 -1.072260 -0.457473   \n",
      "7805 -0.476435 -0.480800 -0.299313  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7806  0.381938 -1.037719 -0.299313  1.544795 -0.514935 -1.072260 -0.457473   \n",
      "7807 -0.381060 -1.037719 -0.299313  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7808  0.000439 -1.037719 -0.299313  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7809  2.766307  0.354577 -0.299313 -0.330552 -0.514935  0.955395 -0.457473   \n",
      "7810  1.621810 -0.202341 -0.299313 -1.737063 -0.514935 -1.072260  2.283513   \n",
      "7811  1.621810 -1.037719 -0.299313 -0.330552 -0.514935 -1.072260 -0.457473   \n",
      "7812  2.957056  0.354577 -0.299313  1.075958 -0.514935 -1.072260 -0.457473   \n",
      "7813 -0.285685 -0.759259 -0.299313 -0.799389 -0.514935  0.955395  2.283513   \n",
      "7814 -0.857934 -0.759259  1.341981 -0.799389 -0.514935 -1.072260 -0.457473   \n",
      "7815 -0.571810 -1.037719 -0.299313  1.075958 -0.514935  0.955395  2.283513   \n",
      "7816  0.191188  1.468414 -1.940608 -0.799389 -0.514935 -1.072260 -0.457473   \n",
      "7817 -0.762559 -0.480800  1.341981  0.607121 -0.514935  0.955395 -0.457473   \n",
      "7818  0.858812  0.911496 -0.299313 -0.330552  1.941994  0.955395  2.283513   \n",
      "7819 -0.762559  1.468414  1.341981 -0.330552 -0.514935 -1.072260 -0.457473   \n",
      "7820 -1.048683  1.468414 -0.299313  0.607121 -0.514935 -1.072260 -0.457473   \n",
      "7821 -0.476435  1.468414 -0.299313  1.075958 -0.514935  0.955395 -0.457473   \n",
      "7822  1.621810  1.468414 -0.299313 -0.799389  1.941994  0.955395  2.283513   \n",
      "7823  1.431060 -1.037719 -1.940608  1.075958 -0.514935  0.955395 -0.457473   \n",
      "7824  0.763437 -0.759259 -0.299313  0.607121 -0.514935 -1.072260 -0.457473   \n",
      "\n",
      "            7         8         9         10        11        12        13  \\\n",
      "0     1.302730 -0.102182  1.408884  1.224143 -0.207346  0.192614 -0.342835   \n",
      "1    -0.767619  0.760576 -0.722414  1.474784 -0.207346  0.192614 -0.342835   \n",
      "2     1.302730  0.760576 -1.432847  0.341115 -0.207346  0.192614 -0.342835   \n",
      "3    -0.767619 -1.396318  1.408884 -0.468649 -0.574019  0.192614 -0.342835   \n",
      "4    -0.767619 -1.396318  1.408884 -0.495641 -0.574019  0.192614 -0.342835   \n",
      "5    -0.767619 -0.533560 -1.432847  0.830829 -0.207346  0.192614 -0.342835   \n",
      "6    -0.767619 -1.827697 -0.722414  1.598176  0.526001 -5.164760  1.667181   \n",
      "7    -0.767619  0.760576 -1.432847  0.001785 -0.574019  0.192614 -0.342835   \n",
      "8     1.302730  1.191954  0.698451 -0.148599 -0.574019  0.192614 -0.342835   \n",
      "9    -0.767619 -1.827697  1.408884  0.618748 -0.574019  0.192614 -0.342835   \n",
      "10   -0.767619 -0.533560  0.698451 -0.619033  0.526001  0.192614 -0.342835   \n",
      "11   -0.767619 -1.827697  1.408884 -0.487929 -0.574019  0.192614 -0.342835   \n",
      "12   -0.767619 -1.396318 -1.432847 -0.364536  0.159328  0.192614 -0.342835   \n",
      "13   -0.767619  0.760576 -0.722414 -0.738570 -0.574019 -5.208139  1.667181   \n",
      "14   -0.767619 -0.533560 -0.011981 -0.900523 -0.574019  0.192614 -0.342835   \n",
      "15   -0.767619  1.623333 -0.011981 -0.376104 -0.574019  0.192614  1.667181   \n",
      "16   -0.767619  0.760576  0.698451  0.186874 -0.207346  0.192614  1.667181   \n",
      "17   -0.767619 -1.396318 -0.011981  0.063482 -0.574019  0.192614 -0.342835   \n",
      "18   -0.767619  0.760576 -0.011981 -0.156311 -0.574019  0.192614 -0.342835   \n",
      "19    1.302730 -0.102182 -0.722414  0.418235 -0.207346  0.192614 -0.342835   \n",
      "20   -0.767619 -1.396318 -1.432847  3.799962  1.626021  0.192614 -0.342835   \n",
      "21    1.302730 -0.102182 -1.432847 -0.692298 -0.574019  0.192614 -0.342835   \n",
      "22    1.302730  0.760576 -0.722414 -0.545769 -0.207346  0.192614 -0.342835   \n",
      "23    1.302730  0.760576  1.408884 -0.491785 -0.207346  0.192614 -0.342835   \n",
      "24   -0.767619 -0.533560  0.698451 -0.761706 -0.574019  0.192614 -0.342835   \n",
      "25    1.302730 -0.102182 -0.722414  1.170158 -0.207346  0.192614 -0.342835   \n",
      "26   -0.767619 -1.827697 -0.011981 -0.214152 -0.574019 -5.191872  3.677196   \n",
      "27   -0.767619 -1.396318  1.408884 -0.684586  1.992695  0.192614 -0.342835   \n",
      "28   -0.767619  0.760576 -1.432847 -0.923659  1.259348  0.192614  1.667181   \n",
      "29   -0.767619  1.191954 -0.011981  2.458068  1.626021  0.192614 -0.342835   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7795 -0.761060 -1.401615  1.428566 -0.443295 -0.569232  0.194770 -0.343695   \n",
      "7796  1.313956 -0.109785  1.428566 -0.877217 -0.569232  0.194770 -0.343695   \n",
      "7797 -0.761060 -1.401615  0.000274 -0.616864  0.165852  0.194770 -0.343695   \n",
      "7798 -0.761060  1.182045 -1.428018 -0.756474 -0.569232  0.194770 -0.343695   \n",
      "7799 -0.761060 -1.832225 -0.713872  0.277394 -0.569232 -5.128133  1.612241   \n",
      "7800 -0.761060  1.612655 -1.428018 -0.398016 -0.201690  0.194770  1.612241   \n",
      "7801 -0.761060  0.751435 -1.428018 -0.277273 -0.569232  0.194770  1.612241   \n",
      "7802 -0.761060 -1.401615  0.000274 -0.786660 -0.569232  0.194770 -0.343695   \n",
      "7803 -0.761060  0.751435  0.714420 -0.635730 -0.569232  0.194770 -0.343695   \n",
      "7804  1.313956 -0.109785  0.714420  2.243251  0.165852  0.194770 -0.343695   \n",
      "7805 -0.761060  0.751435 -1.428018  0.039680 -0.201690  0.194770 -0.343695   \n",
      "7806 -0.761060 -0.540395 -1.428018  1.831968 -0.569232  0.194770  1.612241   \n",
      "7807  1.313956  0.751435 -0.713872 -0.511213  0.165852  0.194770 -0.343695   \n",
      "7808 -0.761060 -1.401615  1.428566 -0.398016 -0.201690  0.194770 -0.343695   \n",
      "7809 -0.761060 -1.832225  1.428566 -0.401789  0.533394  0.194770  1.612241   \n",
      "7810  1.313956  0.751435  1.428566 -0.145209 -0.569232  0.194770 -0.343695   \n",
      "7811  1.313956 -0.540395  0.000274  0.858472  2.371102  0.194770 -0.343695   \n",
      "7812 -0.761060 -1.832225 -0.713872  0.337765 -0.569232  0.194770 -0.343695   \n",
      "7813  1.313956  0.751435 -1.428018 -0.231994 -0.569232  0.194770 -0.343695   \n",
      "7814  1.313956 -0.109785  1.428566  0.341539  1.636019  0.194770 -0.343695   \n",
      "7815 -0.761060 -0.540395 -0.713872 -0.409336  1.636019  0.194770 -0.343695   \n",
      "7816  1.313956  0.751435  0.000274  0.013267 -0.569232  0.194770 -0.343695   \n",
      "7817  1.313956 -1.401615 -1.428018 -0.801753 -0.569232  0.194770 -0.343695   \n",
      "7818 -0.761060 -0.540395  1.428566 -0.820619 -0.569232  0.194770 -0.343695   \n",
      "7819 -0.761060  0.751435 -0.713872 -0.088611 -0.201690  0.194770 -0.343695   \n",
      "7820  1.313956  0.751435 -1.428018  2.363994  0.165852  0.194770 -0.343695   \n",
      "7821 -0.761060 -1.832225 -0.713872 -0.639503  0.165852  0.194770 -0.343695   \n",
      "7822 -0.761060 -0.540395  0.000274 -0.394243  1.268477  0.194770 -0.343695   \n",
      "7823 -0.761060 -0.540395 -0.713872  0.545293 -0.201690  0.194770 -0.343695   \n",
      "7824  1.313956 -0.540395  0.000274  0.032133 -0.569232  0.194770 -0.343695   \n",
      "\n",
      "            14        15        16        17        18        19  \n",
      "0     0.192179  0.840632  1.542744 -0.282113  0.772286  0.845709  \n",
      "1     0.192179 -1.205252 -1.197948 -1.241655 -1.371623 -0.936676  \n",
      "2     0.192179  0.648830  0.721582  0.895506  0.710587  0.333101  \n",
      "3     0.192179  0.840632 -0.237311  0.960929  0.775169  0.845709  \n",
      "4     0.192179  0.840632 -0.237311  0.960929  0.773439  0.845709  \n",
      "5     0.192179  0.840632  0.589081 -0.478383  0.772286  0.845709  \n",
      "6     2.977704 -1.205252 -0.880642 -1.437925 -1.278786 -0.936676  \n",
      "7     0.192179 -1.205252 -1.197948 -1.241655 -1.368164 -0.936676  \n",
      "8     0.192179 -0.118376 -0.662711 -0.325729  0.621209  0.399422  \n",
      "9     0.192179 -1.205252 -0.880642 -1.437925 -1.278786 -0.936676  \n",
      "10    0.192179  0.840632  0.589081 -0.478383  0.771710  0.845709  \n",
      "11    0.192179 -1.205252 -0.880642 -1.437925 -1.273020 -0.936676  \n",
      "12    0.192179  0.840632 -0.237311  0.960929  0.772863  0.845709  \n",
      "13    2.977704 -1.205252 -1.197948 -1.241655 -1.360091 -0.936676  \n",
      "14    0.192179  0.840632  0.589081 -0.478383  0.775746  0.845709  \n",
      "15   -2.593346 -0.757715  1.779852 -1.961311 -1.519817 -2.808871  \n",
      "16   -2.593346 -1.205252 -1.197948 -1.241655 -1.313960 -0.936676  \n",
      "17    0.192179  0.840632 -0.237311  0.960929  0.773439  0.845709  \n",
      "18    0.192179 -1.205252 -1.197948 -1.241655 -1.358938 -0.936676  \n",
      "19    0.192179  0.840632  1.542744 -0.282113  0.769980  0.845709  \n",
      "20    0.192179  0.840632 -0.237311  0.960929  0.773439  0.845709  \n",
      "21    0.192179  0.840632  1.542744 -0.282113  0.763637  0.845709  \n",
      "22    0.192179  0.648830  0.721582  0.895506  0.711740  0.333101  \n",
      "23    0.192179  0.648830  0.721582  0.895506  0.711163  0.333101  \n",
      "24    0.192179 -1.908525 -1.937168  1.506123 -1.486949 -1.253084  \n",
      "25    0.192179  0.840632  1.542744 -0.282113  0.771710  0.845709  \n",
      "26    2.977704 -1.205252  0.294439  1.288046 -1.717601 -2.185727  \n",
      "27    0.192179  0.840632 -0.237311  0.960929  0.773439  0.845709  \n",
      "28   -2.593346 -1.205252 -1.197948 -1.241655 -1.368164 -0.936676  \n",
      "29    0.192179 -2.228194 -1.623348  2.269395 -1.677237 -2.064138  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "7795  0.193983  0.847650 -0.214050  0.939921  0.783738  0.855149  \n",
      "7796  0.193983  0.847650  1.535898 -0.278355  0.780859  0.855149  \n",
      "7797  0.193983  0.847650 -0.214050  0.939921  0.782011  0.855149  \n",
      "7798  0.193983 -0.104198 -0.632255 -0.321102  0.239034  0.407288  \n",
      "7799  2.953828 -1.182960  0.308706  1.260520 -1.706586 -2.186976  \n",
      "7800 -2.565862 -2.198265 -1.950287  2.906261 -1.650734 -2.064958  \n",
      "7801 -2.565862 -1.182960 -1.158440 -1.218779 -1.320226 -0.933521  \n",
      "7802  0.193983  0.847650 -0.214050  0.939921  0.784314  0.855149  \n",
      "7803  0.193983 -1.182960 -1.158440 -1.218779 -1.347289 -0.933521  \n",
      "7804  0.193983  0.847650  1.535898 -0.278355  0.780283  0.855149  \n",
      "7805  0.193983 -1.182960 -1.158440 -1.218779 -1.320226 -0.933521  \n",
      "7806 -2.565862 -1.119503  1.107409  0.042244 -1.580487 -2.424079  \n",
      "7807  0.193983  0.657280  0.728625  0.875801  0.720976  0.340733  \n",
      "7808  0.193983  0.847650 -0.214050  0.939921  0.782011  0.855149  \n",
      "7809 -2.565862 -1.182960 -0.846500 -1.411138 -1.286254 -0.933521  \n",
      "7810  0.193983  0.657280  0.728625  0.875801  0.720976  0.340733  \n",
      "7811  0.193983  0.847650  0.598364 -0.470714  0.778556  0.855149  \n",
      "7812  0.193983 -1.182960 -0.846500 -1.411138 -1.267253 -0.933521  \n",
      "7813  0.193983  0.657280  0.728625  0.875801  0.719249  0.340733  \n",
      "7814  0.193983  0.847650  1.535898 -0.278355  0.780859  0.855149  \n",
      "7815  0.193983  0.847650  0.598364 -0.470714  0.780859  0.855149  \n",
      "7816  0.193983  0.657280  0.728625  0.875801  0.722128  0.340733  \n",
      "7817  0.193983 -1.119503  0.785186  0.469709 -1.555151 -2.424079  \n",
      "7818  0.193983  0.847650  0.598364 -0.470714  0.781435  0.855149  \n",
      "7819  0.193983 -1.182960 -1.158440 -1.218779 -1.328287 -0.933521  \n",
      "7820  0.193983  0.657280  0.728625  0.875801  0.724431  0.340733  \n",
      "7821  0.193983 -1.182960 -0.846500 -1.411138 -1.267253 -0.933521  \n",
      "7822  0.193983  0.847650  0.598364 -0.470714  0.778556  0.855149  \n",
      "7823  0.193983  0.847650  0.598364 -0.470714  0.780859  0.855149  \n",
      "7824  0.193983 -1.119503  1.107409  0.042244 -1.558606 -2.424079  \n",
      "\n",
      "[39128 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw_features_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training the stacking model(XGBoost-Gradient Boosting)\n",
    "model_stack=train_models(stack_X,stack_Y,'stack')\n",
    "\n",
    "#Converting the above list of predictions and raw features (Concatenate) into a dataframe, which will be used to train the blending model.\n",
    "blend_X=pd.concat([raw_features_X, stack_X], axis=1,ignore_index=True)\n",
    "\n",
    "#Training the blending model(XGBoost-Gradient Boosting)\n",
    "model_blend=train_models(blend_X,stack_Y,'blend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialzing the variables that will be used to calculate the area under the curve. (Test Data)\n",
    "metric_logistic_regression_L2=list()\n",
    "metric_logistic_regression_L1=list()\n",
    "metric_decision_tree=list()\n",
    "metric_random_forest=list()\n",
    "metric_multi_layer_perceptron=list()\n",
    "metric_gradient_boosting=list()\n",
    "metric_stack=list()\n",
    "metric_blend=list()\n",
    "blend_X = pd.DataFrame()\n",
    "raw_features_X = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating AUC for all the models (Base Models & Stack Model) on the test data.\n",
    "\n",
    "#Selecting the test data\n",
    "test_Y=test['y']\n",
    "test_X=test.drop(['y'],axis=1)\n",
    "    \n",
    "#Gradient Boosting (XGBoost)\n",
    "#The AUC error (Test Data)\n",
    "[auc,predict_XGB]=cross_validation('Gradient Boosting',model['XGBoost'],xgb.DMatrix(test_X,label=test_Y),test_Y)\n",
    "metric_gradient_boosting=(auc)\n",
    "\n",
    "    \n",
    "#Multi Layer Perceptron\n",
    "#The AUC (Test Data)\n",
    "predict_mlp=list()\n",
    "[auc,predict_multi_layer_perceptron]=cross_validation('Multi Layer Perceptron',model['Multi Layer Perceptron'],test_X,test_Y)\n",
    "metric_multi_layer_perceptron=(auc)\n",
    "#predict_multi_layer_perceptron returns a list of lists containing the predictions, this cannot be converted to a dataframe.\n",
    "#This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "for i in predict_multi_layer_perceptron:\n",
    "        predict_mlp.append(float(i))\n",
    "\n",
    "\n",
    "#Decision Tree)\n",
    "#The AUC (Test Data)\n",
    "[auc,predict_decision_tree]=cross_validation('Decision Tree',model['Decision Tree'],test_X,test_Y)\n",
    "metric_decision_tree=(auc)\n",
    "    \n",
    "    \n",
    "#Random Forest (Deafult=10 Trees)\n",
    "#The AUC (Test Data)\n",
    "[auc,predict_random_forest]=cross_validation('Random Forest',model['Random Forest'],test_X,test_Y)\n",
    "metric_random_forest=(auc)\n",
    "    \n",
    "test_X=preprocessing.StandardScaler().fit_transform(test_X)\n",
    "#Linear Regression\n",
    "#The AUC (Test Data)\n",
    "[auc,predict_linear_regression]=cross_validation('Linear Regression',model['Linear Regression'],test_X,test_Y)\n",
    "metric_linear_regression=(auc)\n",
    "    \n",
    "#Logistic Regression (Default=l2)\n",
    "#The AUC (Test Data)\n",
    "[auc,predict_logistic_regression_L2]=cross_validation('L2',model['L2'],test_X,test_Y)\n",
    "metric_logistic_regression_L2=(auc)\n",
    "\n",
    "#Logistic Regression-L1\n",
    "#The AUC (Test Data)\n",
    "[auc,predict_logistic_regression_L1]=cross_validation('L1',model['L1'],test_X,test_Y)\n",
    "metric_logistic_regression_L1=(auc)\n",
    "\n",
    "#Building a list that contains all the predictions of the base models.\n",
    "predict_list=[predict_XGB,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    \n",
    "#Stacking (XGBoost - Gradient Boosting)\n",
    "dstack_X=build_data_frame(predict_list) #Converting the list of predictions into a dataframe.\n",
    "[auc,predict_stack]=cross_validation('Gradient Boosting',model_stack['Stack'],xgb.DMatrix(dstack_X,label=test_Y),test_Y)\n",
    "metric_stack=(auc)    \n",
    "\n",
    "#Blending (XGBoost - Gradient Boosting)\n",
    "raw_features_X=raw_features_X.append(test_X.tolist())\n",
    "blend_X=pd.concat([raw_features_X, dstack_X], axis=1,ignore_index=True)#Converting the above list of predictions and raw features (Concatenate) into a dataframe\n",
    "[auc,predict_blend]=cross_validation('Gradient Boosting',model_blend['Blend'],xgb.DMatrix(blend_X,label=test_Y),test_Y)\n",
    "metric_blend=(auc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC (Linear Regression)\n",
      " 0.924578342178\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.926204502062\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.926270182615\n",
      " AUC (Decision Tree)\n",
      " 0.937767676562\n",
      " AUC (Random Forest)\n",
      " 0.906800428509\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.869514258342\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.95506091305\n",
      " AUC (Stacking)\n",
      " 0.949515436062\n",
      " AUC (Blending)\n",
      " 0.950945686713\n"
     ]
    }
   ],
   "source": [
    "print (' AUC (Linear Regression)\\n',metric_linear_regression)\n",
    "print (' AUC (Logistic Regression - L2)\\n',metric_logistic_regression_L2)\n",
    "print (' AUC (Logistic Regression - L1)\\n',metric_logistic_regression_L1)\n",
    "print (' AUC (Decision Tree)\\n',metric_decision_tree)\n",
    "print (' AUC (Random Forest)\\n',metric_random_forest)\n",
    "print (' AUC (Multi Layer Perceptron)\\n',metric_multi_layer_perceptron)\n",
    "print (' AUC (Gradient Boosting - XGBoost)\\n',metric_gradient_boosting)\n",
    "print (' AUC (Stacking)\\n',metric_stack)\n",
    "print (' AUC (Blending)\\n',metric_blend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
