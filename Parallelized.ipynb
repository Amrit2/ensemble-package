{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading the data, into a Data Frame.\n",
    "Data = pd.read_csv('/home/prajwal/Desktop/bank-additional/bank-additional-full.csv',delimiter=';',header=0)\n",
    "\n",
    "#Encoding the data, encoding the string values into numerical values.\n",
    "encode = preprocessing.LabelEncoder()\n",
    "\n",
    "#Selcting the columns of string data type\n",
    "names = Data.select_dtypes(include = ['object'])\n",
    "\n",
    "#Function that encodes the string values to numerical values.\n",
    "def enc(data,column):\n",
    "    data[column] = encode.fit_transform(data[column])\n",
    "    return data\n",
    "for column in names:\n",
    "        Data = enc(Data,column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        base_second_level_models_initialize()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_stack_blend()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing datasets (Stratified Split)\n",
    "def data_split():\n",
    "    \n",
    "    global Data\n",
    "    global test\n",
    "    Data, test = train_test_split(Data, test_size = 0.1, stratify = Data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used to convert the predictions of the base models into a DataFrame.\n",
    "def build_data_frame(data):\n",
    "    \n",
    "    data_frame = pd.DataFrame(data).T\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_initialize():\n",
    "    \n",
    "    #Initializing the test dataset.\n",
    "    test = pd.DataFrame()\n",
    "    \n",
    "    global stack_X\n",
    "    global stack_Y\n",
    "    \n",
    "    #Initializing two data frames that will be used as training data for the stacked model.\n",
    "    \n",
    "    #The data frame will contain the predictions of the base models.\n",
    "    stack_X = pd.DataFrame() \n",
    "    #The data frame will contain the calss labels of the base models.\n",
    "    stack_Y = pd.DataFrame() \n",
    "    \n",
    "    global blend_X\n",
    "    global raw_features_X\n",
    "    \n",
    "    #Initializing two data frames that will be used as training data for the blending model.\n",
    "    #The data frames will contain the predictions and raw features  of the base models.\n",
    "    blend_X = pd.DataFrame() \n",
    "    #The data frames will contain the raw features  of the data, which will be concatenated with the predictions.\n",
    "    raw_features_X = pd.DataFrame() \n",
    "    \n",
    "    global test_blend_X\n",
    "    global test_raw_features_X\n",
    "    global test_stack_X \n",
    "    global test_stack_Y\n",
    "    \n",
    "    #Initializing the dataframes that will be used for testing the stacking and blending models.\n",
    "    test_blend_X = pd.DataFrame()\n",
    "    test_raw_features_X = pd.DataFrame()\n",
    "    test_stack_X = pd.DataFrame()\n",
    "    test_stack_Y = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the parameters for the XGBoost (Gradient Boosting) Algorithm.\n",
    "def param_set():\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = 0.3\n",
    "    param['gamma'] = 0\n",
    "    param['max_depth'] = 6\n",
    "    param['min_child_weight'] = 1\n",
    "    param['max_delta_step'] = 0\n",
    "    param['subsample'] = 1\n",
    "    param['colsample_bytree'] = 1\n",
    "    param['silent'] = 1\n",
    "    param['seed'] = 0\n",
    "    param['base_score'] = 0.5\n",
    "    param['lambda_bias'] = 1\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_second_level_models_initialize():\n",
    "    \n",
    "    global gradient_boosting\n",
    "    global multi_layer_perceptron\n",
    "    global decision_tree\n",
    "    global random_forest\n",
    "    global linear_regression\n",
    "    global logistic_regression_L1\n",
    "    global logistic_regression_L2\n",
    "    \n",
    "    #Initializing the base models.\n",
    "    gradient_boosting = xgb.Booster()\n",
    "    multi_layer_perceptron = Sequential()\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=6)\n",
    "    random_forest = RandomForestClassifier()\n",
    "    linear_regression = linear_model.LinearRegression()\n",
    "    logistic_regression_L1 = linear_model.LogisticRegression(penalty = 'l1')\n",
    "    logistic_regression_L2 = linear_model.LogisticRegression(penalty = 'l2')\n",
    "    \n",
    "    global stack\n",
    "    global blend\n",
    "    #Initializing the second level models.\n",
    "    stack=xgb.Booster()\n",
    "    blend=xgb.Booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trains the Gradient Boosting model.\n",
    "def train_gradient_boosting(train_X,train_Y):\n",
    "\n",
    "    param = param_set()\n",
    "    dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "    gradient_boosting = xgb.train(param, dtrain)\n",
    "    return gradient_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Multi Layer Perceptron model.\n",
    "def train_multi_layer_perceptron(train_X,train_Y):\n",
    "    \n",
    "    multi_layer_perceptron.add(Dense(output_dim = 64, input_dim = 20, init = 'uniform', activation = 'sigmoid'))\n",
    "    multi_layer_perceptron.add(Dense(output_dim = 1, input_dim = 64,activation = 'sigmoid',))\n",
    "    multi_layer_perceptron.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "    multi_layer_perceptron.fit(train_X.as_matrix(), train_Y.as_matrix(), nb_epoch = 5, batch_size = 128)\n",
    "    return multi_layer_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Decision Tree model.\n",
    "def train_decision_tree(train_X,train_Y):\n",
    "    \n",
    "    decision_tree.fit(train_X,train_Y)\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Random Forest model.\n",
    "def train_random_forest(train_X,train_Y):\n",
    "    \n",
    "    random_forest.fit(train_X,train_Y)\n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Linear Regression model.\n",
    "def train_linear_regression(train_X,train_Y):\n",
    "    \n",
    "    #Scaling the data\n",
    "    train_X = preprocessing.StandardScaler().fit_transform(train_X)\n",
    "    linear_regression.fit(train_X,train_Y)\n",
    "    return linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Logistic Regression (L1) model.\n",
    "def train_logistic_regression_L1(train_X,train_Y):\n",
    "    \n",
    "    #Scaling the data\n",
    "    train_X = preprocessing.StandardScaler().fit_transform(train_X)\n",
    "    logistic_regression_L1.fit(train_X,train_Y)\n",
    "    return logistic_regression_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Logistic Regression (L1) model.\n",
    "def train_logistic_regression_L2(train_X,train_Y):\n",
    "    \n",
    "    #Scaling the data\n",
    "    train_X = preprocessing.StandardScaler().fit_transform(train_X)\n",
    "    logistic_regression_L2.fit(train_X,train_Y)\n",
    "    return logistic_regression_L2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Stacking model (Gradient Boosting - XGBoost)\n",
    "def train_stack_model(train_X,train_Y):\n",
    "    \n",
    "    param = param_set()\n",
    "    dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "    stack = xgb.train(param, dtrain)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the blending model (Gradient Boosting - XGBoost)\n",
    "def train_blend_model(train_X,train_Y): \n",
    "    \n",
    "    param = param_set()\n",
    "    dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "    blend = xgb.train(param, dtrain)\n",
    "    return blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_gradient_boosting(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    predict = gradient_boosting.predict(xgb.DMatrix(cross_val_X, label = cross_val_Y))\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_multi_layer_perceptron(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    predict = multi_layer_perceptron.predict_on_batch(cross_val_X)\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_decision_tree(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    global decision_tree\n",
    "    predict = decision_tree.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_random_forest(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    predict = random_forest.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_linear_regression(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    cross_val_X = preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    predict = linear_regression.predict(cross_val_X)\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_logistic_regression_L1(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    cross_val_X = preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    predict = logistic_regression_L1.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_logistic_regression_L2(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    cross_val_X = preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    predict = logistic_regression_L2.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_val_stack(cross_val_X,cross_val_Y):\n",
    "\n",
    "    predict = stack.predict(xgb.DMatrix(cross_val_X,label = cross_val_Y))\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_blend(cross_val_X,cross_val_Y):\n",
    "\n",
    "    predict = blend.predict(xgb.DMatrix(cross_val_X,label = cross_val_Y))\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perfroms weighted average of the predictions of the base models.\n",
    "def weighted_average(data_frame_predictions, cross_val_Y,weight):\n",
    "    \n",
    "    weighted_avg_predictions=np.average(data_frame_predictions,axis=1,weights=weight)\n",
    "    auc = roc_auc_score(cross_val_Y,weighted_avg_predictions)\n",
    "    return [auc,weighted_avg_predictions]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining the objective. Appropriate weights need to be calculated to minimize the loss.\n",
    "def objective(space):\n",
    "    \n",
    "    global stack_X\n",
    "    weighted_avg_predictions=np.average(stack_X,axis=1,weights=[space['w1'],space['w2'],space['w3']\n",
    "                                                                              ,space['w4'],space['w5'],\n",
    "                                                                              space['w6'],space['w7']])\n",
    "    \n",
    "    \n",
    "    global stack_Y\n",
    "    auc = roc_auc_score(stack_Y,weighted_avg_predictions)\n",
    "    return{'loss':1-auc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assigning the weights that need to be checked, for minimizing the objective (Loss)\n",
    "def assign_space():\n",
    "    \n",
    "    global space\n",
    "    space ={\n",
    "        'w1': hp.choice(\"x_w1\", range(10)),\n",
    "        'w2': hp.choice('x_w2', range(10)),\n",
    "        'w3': hp.choice('x_w3', range(10)),\n",
    "        'w4': hp.choice('x_w4', range(10)),\n",
    "        'w5': hp.choice('x_w5', range(10)),\n",
    "        'w6': hp.choice('x_w6', range(10)),\n",
    "        'w7': hp.choice('x_w7', range(10))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function that finds the best possible combination of weights for performing the weighted predictions.\n",
    "def get_weights():\n",
    "    \n",
    "    assign_space()\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials)\n",
    "    \n",
    "    best_weights=list()\n",
    "    \n",
    "    #Arranging the weights in order of the respective models, and then returning the list of weights.\n",
    "    for key in sorted(best):\n",
    "        best_weights.append(best[key])\n",
    "    \n",
    "    return best_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_initialize():\n",
    "    \n",
    "    global metric_linear_regression\n",
    "    global metric_logistic_regression_L2\n",
    "    global metric_logistic_regression_L1\n",
    "    global metric_decision_tree\n",
    "    global metric_random_forest\n",
    "    global metric_gradient_boosting\n",
    "    global metric_multi_layer_perceptron\n",
    "    global metric_stacking\n",
    "    global metric_blending\n",
    "    global metric_weighted_average\n",
    "    \n",
    "    #Initialzing the variables that will be used to calculate the area under the curve on the given data.\n",
    "    metric_linear_regression = list()\n",
    "    metric_logistic_regression_L2 = list()\n",
    "    metric_logistic_regression_L1 = list()\n",
    "    metric_decision_tree = list()\n",
    "    metric_random_forest = list()\n",
    "    metric_multi_layer_perceptron = list()\n",
    "    metric_gradient_boosting = list()\n",
    "    metric_weighted_average = list()\n",
    "    metric_stacking = list()\n",
    "    metric_blending = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The list of base model functions (Training).\n",
    "train_base_model_list = [train_gradient_boosting,train_multi_layer_perceptron,train_decision_tree,train_random_forest,\n",
    "                 train_linear_regression,train_logistic_regression_L1,train_logistic_regression_L2]\n",
    "\n",
    "#The list of base model functions (Cross Validation).\n",
    "cross_val_base_model_list = [cross_val_gradient_boosting,cross_val_multi_layer_perceptron,cross_val_decision_tree\n",
    "                           ,cross_val_random_forest,cross_val_linear_regression,cross_val_logistic_regression_L1\n",
    "                           ,cross_val_logistic_regression_L2]\n",
    "\n",
    "#The list of second level model functions.\n",
    "cross_val_second_level_model = [cross_val_stack,cross_val_blend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_cross_val_base_models():\n",
    "    \n",
    "    #Cross Validation using Stratified K Fold\n",
    "    train, cross_val = train_test_split(Data, test_size = 0.5, stratify = Data['y'])\n",
    "    \n",
    "    #Training the base models, and calculating AUC on the cross validation data.\n",
    "    #Selecting the data (Traing Data & Cross Validation Data)\n",
    "    train_Y = train['y']\n",
    "    train_X = train.drop(['y'],axis=1)\n",
    "    cross_val_Y = cross_val['y']\n",
    "    cross_val_X = cross_val.drop(['y'],axis=1)\n",
    "    \n",
    "    global gradient_boosting\n",
    "    global multi_layer_perceptron\n",
    "    global decision_tree\n",
    "    global random_forest\n",
    "    global linear_regression\n",
    "    global logistic_regression_L1\n",
    "    global logistic_regression_L2\n",
    "\n",
    "    #Training the base models parallely, the resulting models are stored which will be used for cross validation.\n",
    "    [gradient_boosting,multi_layer_perceptron,decision_tree,random_forest,linear_regression,logistic_regression_L1\n",
    "     ,logistic_regression_L2] = (Parallel(n_jobs = -1)(delayed(function)(train_X, train_Y)\\\n",
    "                                                   for function in train_base_model_list))\n",
    "    \n",
    "    #Computing the AUC and Predictions of all the base models on the cross validation data parallely.\n",
    "    auc_predict_cross_val = (Parallel(n_jobs = -1)(delayed(function)(cross_val_X,cross_val_Y)\\\n",
    "                                               for function in cross_val_base_model_list))\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Cross Validation Data)\n",
    "    auc,predict_gradient_boosting = auc_predict_cross_val[0][0],auc_predict_cross_val[0][1]\n",
    "    metric_gradient_boosting.append(auc)\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Cross Validation Data)\n",
    "    predict_mlp=list()\n",
    "    auc,predict_multi_layer_perceptron = auc_predict_cross_val[1][0],auc_predict_cross_val[1][1]\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "    \n",
    "    #predict_multi_layer_perceptron returns a list of lists containing the predictions,\n",
    "    #this cannot be converted to a dataframe.\n",
    "    #This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "    for i in predict_multi_layer_perceptron:\n",
    "        predict_mlp.append(float(i))\n",
    "    \n",
    "    #Decision Tree)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_decision_tree = auc_predict_cross_val[2][0],auc_predict_cross_val[2][1]\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_random_forest = auc_predict_cross_val[3][0],auc_predict_cross_val[3][1]\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    #Linear Regression\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_linear_regression = auc_predict_cross_val[4][0],auc_predict_cross_val[4][1]\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_logistic_regression_L1 = auc_predict_cross_val[5][0],auc_predict_cross_val[5][1]\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L2\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_logistic_regression_L2 = auc_predict_cross_val[6][0],auc_predict_cross_val[6][1]\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Building a list that contains all the predictions of the base models.\n",
    "    predict_list = [predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    \n",
    "    #Converting the above list of predictions into a dataframe, which will be used to train the stacking model.\n",
    "    global stack_X\n",
    "    stack_X = stack_X.append(build_data_frame(predict_list))\n",
    "    \n",
    "    #Building a list that contains all the raw features, used as cross validation data for the base models.\n",
    "    global raw_features_X\n",
    "    raw_features_X = raw_features_X.append(cross_val_X,ignore_index=True)\n",
    "    \n",
    "    #Storing the cross validation dataset labels in the variable stack_Y, \n",
    "    #which will be used later to train the stacking and blending models.\n",
    "    global stack_Y\n",
    "    stack_Y = cross_val_Y  \n",
    "    \n",
    "    #Performing a weighted average of all the base models and calculating the resulting AUC.\n",
    "    global weight\n",
    "    weight = get_weights()\n",
    "    \n",
    "    auc,predict_weighted_average = weighted_average(stack_X,stack_Y,weight)\n",
    "    metric_weighted_average.append(auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_metric_cross_val(n):\n",
    "    \n",
    "    #Calculating the average AUC across all the AUC computed on the cross validation folds.\n",
    "    avg_linear_regression = np.mean(metric_linear_regression)\n",
    "    avg_logistic_regression_L2 = np.mean(metric_logistic_regression_L2)\n",
    "    avg_logistic_regression_L1 = np.mean(metric_logistic_regression_L1)\n",
    "    avg_decision_tree = np.mean(metric_decision_tree)\n",
    "    avg_random_forest = np.mean(metric_random_forest)\n",
    "    avg_multi_layer_perceptron = np.mean(metric_multi_layer_perceptron)\n",
    "    avg_gradient_boosting = np.mean(metric_gradient_boosting)\n",
    "    \n",
    "    #Printing the AUC for the base models.\n",
    "    print('\\nStart Cross Validation Sample',n,'\\n')\n",
    "    print (' AUC (Linear Regression)\\n',avg_linear_regression)\n",
    "    print (' AUC (Logistic Regression - L2)\\n',avg_logistic_regression_L2)\n",
    "    print (' AUC (Logistic Regression - L1)\\n',avg_logistic_regression_L1)\n",
    "    print (' AUC (Decision Tree)\\n',avg_decision_tree)\n",
    "    print (' AUC (Random Forest)\\n',avg_random_forest)\n",
    "    print (' AUC (Multi Layer Perceptron)\\n',avg_multi_layer_perceptron)\n",
    "    print (' AUC (Weighted Average)\\n',metric_weighted_average)\n",
    "    print (' AUC (Gradient Boosting - XGBoost)\\n',avg_gradient_boosting)\n",
    "    print('\\nEnd Cross Validation Sample',n,'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_stack_blend():\n",
    "    \n",
    "    #Converting the above list of predictions and raw features (Concatenate) into a dataframe, \n",
    "    #which will be used to train the blending model.\n",
    "    global blend_X\n",
    "    blend_X = pd.concat([raw_features_X, stack_X], axis = 1,ignore_index = True)\n",
    "    \n",
    "    #Training the Stacking and Blending models parallely using the \n",
    "    #predictions of base models on the cross validation data.\n",
    "    global stack\n",
    "    global blend\n",
    "    function_param = [(train_stack_model,stack_X,stack_Y),(train_blend_model,blend_X,stack_Y)]\n",
    "    [stack,blend] = Parallel(n_jobs = -1)(delayed(model_function)(train_X,train_Y)\\\n",
    "                                        for model_function,train_X,train_Y in function_param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_metric_test(n):\n",
    "    \n",
    "    print('\\nStart Test Sample',n,'\\n')\n",
    "    #Printing the AUC for all the models. (Test Data)\n",
    "    print (' AUC (Linear Regression)\\n',metric_linear_regression)\n",
    "    print (' AUC (Logistic Regression - L2)\\n',metric_logistic_regression_L2)\n",
    "    print (' AUC (Logistic Regression - L1)\\n',metric_logistic_regression_L1)\n",
    "    print (' AUC (Decision Tree)\\n',metric_decision_tree)\n",
    "    print (' AUC (Random Forest)\\n',metric_random_forest)\n",
    "    print (' AUC (Multi Layer Perceptron)\\n',metric_multi_layer_perceptron)\n",
    "    print (' AUC (Weighted Average)\\n',metric_weighted_average)\n",
    "    print (' AUC (Gradient Boosting - XGBoost)\\n',metric_gradient_boosting)\n",
    "    print (' AUC (Stacking)\\n',metric_stacking)\n",
    "    print (' AUC (Blending)\\n',metric_blending)\n",
    "    print('\\nEnd Test Sample',n,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Base and Second Level Models on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    \n",
    "    #Training the base models, and calculating AUC on the test data.\n",
    "    #Selecting the data (Test Data)\n",
    "    test_Y = test['y']\n",
    "    test_X = test.drop(['y'],axis=1)\n",
    "    \n",
    "    #Computing the AUC and Predictions of all the base models on the test data parallely.\n",
    "    auc_predict_test = (Parallel(n_jobs = -1)(delayed(function)(test_X,test_Y)\\\n",
    "                                          for function in cross_val_base_model_list))\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Test Data)\n",
    "    auc,predict_gradient_boosting = auc_predict_test[0][0],auc_predict_test[0][1]\n",
    "    metric_gradient_boosting.append(auc)\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Test Data)\n",
    "    predict_mlp=list()\n",
    "    auc,predict_multi_layer_perceptron = auc_predict_test[1][0],auc_predict_test[1][1]\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "    \n",
    "    #predict_multi_layer_perceptron returns a list of lists containing the predictions, \n",
    "    #this cannot be converted to a dataframe.\n",
    "    #This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "    for i in predict_multi_layer_perceptron:\n",
    "        predict_mlp.append(float(i))\n",
    "    \n",
    "    #Decision Tree)\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_decision_tree = auc_predict_test[2][0],auc_predict_test[2][1]\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_random_forest = auc_predict_test[3][0],auc_predict_test[3][1]\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    #Linear Regression\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_linear_regression = auc_predict_test[4][0],auc_predict_test[4][1]\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_logistic_regression_L1 = auc_predict_test[5][0],auc_predict_test[5][1]\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L2\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_logistic_regression_L2 = auc_predict_test[6][0],auc_predict_test[6][1]\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Building a list that contains all the predictions of the base models.\n",
    "    predict_list = [predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    global test_stack_X\n",
    "    global test_raw_features_X\n",
    "    global test_blend_X\n",
    "    \n",
    "    test_stack_X = build_data_frame(predict_list)\n",
    "    #Converting the list of predictions into a dataframe.\n",
    "    test_raw_features_X = test_raw_features_X.append(test_X,ignore_index = True)\n",
    "    #Converting the above list of predictions and raw features (Concatenate) into a dataframe\n",
    "    test_blend_X = pd.concat([test_raw_features_X, test_stack_X], axis = 1,ignore_index = True)\n",
    "\n",
    "    #Performing a weighted average of all the base models and calculating the resulting AUC.\n",
    "    auc,predict_weighted_average = weighted_average(test_stack_X,test_Y,weight)\n",
    "    metric_weighted_average.append(auc)\n",
    "        \n",
    "    #Computing the AUC and Predictions of the Stacking and Blending models on the test data parallely.\n",
    "    auc_predict_test_second_level = Parallel(n_jobs = -1)(delayed(function)(test_X, test_Y) \\\n",
    "                                                      for function,test_X in \\\n",
    "                                                      ((cross_val_second_level_model[0],test_stack_X),\\\n",
    "                                                       (cross_val_second_level_model[1],test_blend_X)))\n",
    "\n",
    "    #Stacking (XGBoost - Gradient Boosting)\n",
    "    auc,predict_stack = auc_predict_test_second_level[0][0],auc_predict_test_second_level[0][1]\n",
    "    metric_stacking.append(auc)    \n",
    "\n",
    "    #Blending (XGBoost - Gradient Boosting)\n",
    "    auc,predict_blend = auc_predict_test_second_level[1][0],auc_predict_test_second_level[1][1]\n",
    "    metric_blending.append(auc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.3362 - acc: 0.8873     \n",
      "Epoch 2/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.3053 - acc: 0.8873     \n",
      "Epoch 3/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.2832 - acc: 0.8873     \n",
      "Epoch 4/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.2736 - acc: 0.8873     \n",
      "Epoch 5/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.2657 - acc: 0.8873     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.92611364916\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.927504363837\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.927529639499\n",
      " AUC (Decision Tree)\n",
      " 0.919517531292\n",
      " AUC (Random Forest)\n",
      " 0.914502176036\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.871434588334\n",
      " AUC (Weighted Average)\n",
      " [0.9440523222509456]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.943586338633\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92831088730600497]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.92944065757818761]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92945127128638139]\n",
      " AUC (Decision Tree)\n",
      " [0.91879009623095431]\n",
      " AUC (Random Forest)\n",
      " [0.91348678003679418]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.87601449360818906]\n",
      " AUC (Weighted Average)\n",
      " [0.94462297749893864]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94287820180197179]\n",
      " AUC (Stacking)\n",
      " [0.94266946554082731]\n",
      " AUC (Blending)\n",
      " [0.94425709939148095]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "Epoch 1/5\n",
      "16681/16681 [==============================] - 0s - loss: 0.3818 - acc: 0.8624     \n",
      "Epoch 2/5\n",
      "16681/16681 [==============================] - 0s - loss: 0.2850 - acc: 0.8875     \n",
      "Epoch 3/5\n",
      "16681/16681 [==============================] - 0s - loss: 0.2673 - acc: 0.8901     \n",
      "Epoch 4/5\n",
      "16681/16681 [==============================] - 0s - loss: 0.2607 - acc: 0.8952     \n",
      "Epoch 5/5\n",
      "16681/16681 [==============================] - 0s - loss: 0.2570 - acc: 0.9006     \n",
      "\n",
      "Start Cross Validation Sample 1 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.929767646433\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.933078459328\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.933104652155\n",
      " AUC (Decision Tree)\n",
      " 0.921718610441\n",
      " AUC (Random Forest)\n",
      " 0.917974851866\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.879798078291\n",
      " AUC (Weighted Average)\n",
      " [0.94742571070649872]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.945195257549\n",
      "\n",
      "End Cross Validation Sample 1 \n",
      "\n",
      "\n",
      "Start Test Sample 1 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92165926438861745]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.92470843074129949]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92475280076694688]\n",
      " AUC (Decision Tree)\n",
      " [0.9102154346589546]\n",
      " AUC (Random Forest)\n",
      " [0.90996157992205429]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.86490127305604736]\n",
      " AUC (Weighted Average)\n",
      " [0.93883410120148214]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.93778667764521717]\n",
      " AUC (Stacking)\n",
      " [0.93537469395592965]\n",
      " AUC (Blending)\n",
      " [0.93943709712380397]\n",
      "\n",
      "End Test Sample 1 \n",
      "\n",
      "Epoch 1/5\n",
      "15012/15012 [==============================] - 0s - loss: 0.4099 - acc: 0.8327     \n",
      "Epoch 2/5\n",
      "15012/15012 [==============================] - 0s - loss: 0.3000 - acc: 0.8874     \n",
      "Epoch 3/5\n",
      "15012/15012 [==============================] - 0s - loss: 0.2928 - acc: 0.8874     \n",
      "Epoch 4/5\n",
      "15012/15012 [==============================] - 0s - loss: 0.2854 - acc: 0.8874     \n",
      "Epoch 5/5\n",
      "15012/15012 [==============================] - 0s - loss: 0.2785 - acc: 0.8878     \n",
      "\n",
      "Start Cross Validation Sample 2 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.928950045149\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.928770708799\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.928824287753\n",
      " AUC (Decision Tree)\n",
      " 0.911866615304\n",
      " AUC (Random Forest)\n",
      " 0.916690807529\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.863166098043\n",
      " AUC (Weighted Average)\n",
      " [0.94783931214388528]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.948055869665\n",
      "\n",
      "End Cross Validation Sample 2 \n",
      "\n",
      "\n",
      "Start Test Sample 2 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.93376752391012219]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.93437919909173872]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.9344690192358821]\n",
      " AUC (Decision Tree)\n",
      " [0.91364691342056648]\n",
      " AUC (Random Forest)\n",
      " [0.91830723159944516]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.86395975698261807]\n",
      " AUC (Weighted Average)\n",
      " [0.95053065741159914]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94816614211702488]\n",
      " AUC (Stacking)\n",
      " [0.94926868438638468]\n",
      " AUC (Blending)\n",
      " [0.94825775866405115]\n",
      "\n",
      "End Test Sample 2 \n",
      "\n",
      "Epoch 1/5\n",
      "13511/13511 [==============================] - 0s - loss: 0.3702 - acc: 0.8795     \n",
      "Epoch 2/5\n",
      "13511/13511 [==============================] - 0s - loss: 0.3050 - acc: 0.8874     \n",
      "Epoch 3/5\n",
      "13511/13511 [==============================] - 0s - loss: 0.2807 - acc: 0.8874     \n",
      "Epoch 4/5\n",
      "13511/13511 [==============================] - 0s - loss: 0.2710 - acc: 0.8874     \n",
      "Epoch 5/5\n",
      "13511/13511 [==============================] - 0s - loss: 0.2651 - acc: 0.8940     \n",
      "\n",
      "Start Cross Validation Sample 3 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.92927444222\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.930666843205\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.930629960951\n",
      " AUC (Decision Tree)\n",
      " 0.924573927765\n",
      " AUC (Random Forest)\n",
      " 0.911780991972\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.863756927205\n",
      " AUC (Weighted Average)\n",
      " [0.94658523488844193]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.945264406301\n",
      "\n",
      "End Cross Validation Sample 3 \n",
      "\n",
      "\n",
      "Start Test Sample 3 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.93232567692085666]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.93353353242226089]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.93351799016397097]\n",
      " AUC (Decision Tree)\n",
      " [0.93137537884254584]\n",
      " AUC (Random Forest)\n",
      " [0.91883832720894343]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.86709981460306174]\n",
      " AUC (Weighted Average)\n",
      " [0.94906690942193905]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94621768042896626]\n",
      " AUC (Stacking)\n",
      " [0.94825593658758611]\n",
      " AUC (Blending)\n",
      " [0.95270823850705499]\n",
      "\n",
      "End Test Sample 3 \n",
      "\n",
      "Epoch 1/5\n",
      "12159/12159 [==============================] - 0s - loss: 0.3304 - acc: 0.8873     \n",
      "Epoch 2/5\n",
      "12159/12159 [==============================] - 0s - loss: 0.3105 - acc: 0.8873     \n",
      "Epoch 3/5\n",
      "12159/12159 [==============================] - 0s - loss: 0.3092 - acc: 0.8873     \n",
      "Epoch 4/5\n",
      "12159/12159 [==============================] - 0s - loss: 0.2996 - acc: 0.8873     \n",
      "Epoch 5/5\n",
      "12159/12159 [==============================] - 0s - loss: 0.2924 - acc: 0.8873     \n",
      "\n",
      "Start Cross Validation Sample 4 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.925747346489\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.927646678798\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.92762824459\n",
      " AUC (Decision Tree)\n",
      " 0.908998532028\n",
      " AUC (Random Forest)\n",
      " 0.915967981978\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.872177130758\n",
      " AUC (Weighted Average)\n",
      " [0.94589522604736742]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.943565920053\n",
      "\n",
      "End Cross Validation Sample 4 \n",
      "\n",
      "\n",
      "Start Test Sample 4 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92198503762532635]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.92438186963866531]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92435855948750578]\n",
      " AUC (Decision Tree)\n",
      " [0.91193287773414378]\n",
      " AUC (Random Forest)\n",
      " [0.90488155700840267]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.86655418376077753]\n",
      " AUC (Weighted Average)\n",
      " [0.94330834119479612]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.93658953291941827]\n",
      " AUC (Stacking)\n",
      " [0.9398125041135561]\n",
      " AUC (Blending)\n",
      " [0.94175410258660408]\n",
      "\n",
      "End Test Sample 4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_generation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(Parallel(n_jobs=-1)(delayed(sample_generation)(n) for n in range(4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
