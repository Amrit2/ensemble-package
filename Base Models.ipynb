{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode = preprocessing.LabelEncoder()\n",
    "Data = pd.read_csv('/home/prajwal/Desktop/bank-additional/bank-additional-full.csv',delimiter=';',header=0)\n",
    "#print (Data)\n",
    "Data.job = encode.fit_transform(Data.job)\n",
    "Data.marital = encode.fit_transform(Data.marital)\n",
    "Data.education = encode.fit_transform(Data.education)\n",
    "Data.default = encode.fit_transform(Data.default)\n",
    "Data.housing = encode.fit_transform(Data.housing)\n",
    "Data.loan = encode.fit_transform(Data.loan)\n",
    "Data.contact = encode.fit_transform(Data.contact)\n",
    "Data.month = encode.fit_transform(Data.month)\n",
    "Data.day_of_week = encode.fit_transform(Data.day_of_week)\n",
    "Data.poutcome = encode.fit_transform(Data.poutcome)\n",
    "Data.y = encode.fit_transform(Data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_LinearRegression=list()\n",
    "avg_LinearRegeression=0\n",
    "mean_L2=list()\n",
    "avg_L2=0\n",
    "mean_L1=list()\n",
    "avg_L1=0\n",
    "mean_DT=list()\n",
    "avg_DT=0\n",
    "mean_RF=list()\n",
    "avg_RF=0\n",
    "mean_MLP=list()\n",
    "avg_MLP=0\n",
    "mean_GB=list()\n",
    "avg_GB=0\n",
    "mean_ET=list()\n",
    "avg_ET=0\n",
    "mean_XGB=list()\n",
    "avg_XGB=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variance_L2=list()\n",
    "variance_L1=list()\n",
    "variance_DT=list()\n",
    "variance_RF=list()\n",
    "variance_MLP=list()\n",
    "variance_GB=list()\n",
    "variance_ET=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(Data['y'], n_folds=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0955 - acc: 0.8866     \n",
      "Epoch 2/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0797 - acc: 0.8918     \n",
      "Epoch 3/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0772 - acc: 0.9002     \n",
      "Epoch 4/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0766 - acc: 0.8998     \n",
      "Epoch 5/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0767 - acc: 0.9016     \n",
      "Epoch 1/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0920 - acc: 0.8811     \n",
      "Epoch 2/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0770 - acc: 0.8973     \n",
      "Epoch 3/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0762 - acc: 0.8984     \n",
      "Epoch 4/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0764 - acc: 0.9002     \n",
      "Epoch 5/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0761 - acc: 0.9012     \n",
      "Epoch 1/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0844 - acc: 0.8889     \n",
      "Epoch 2/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0772 - acc: 0.8989     \n",
      "Epoch 3/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0756 - acc: 0.9004     \n",
      "Epoch 4/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0765 - acc: 0.8994     \n",
      "Epoch 5/5\n",
      "32950/32950 [==============================] - 1s - loss: 0.0765 - acc: 0.8990     \n",
      "Epoch 1/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0904 - acc: 0.8759     \n",
      "Epoch 2/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0754 - acc: 0.9013     \n",
      "Epoch 3/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0750 - acc: 0.9019     \n",
      "Epoch 4/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0749 - acc: 0.9019     \n",
      "Epoch 5/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0749 - acc: 0.9031     \n",
      "Epoch 1/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0831 - acc: 0.8894     \n",
      "Epoch 2/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0765 - acc: 0.8998     \n",
      "Epoch 3/5\n",
      "32951/32951 [==============================] - 2s - loss: 0.0754 - acc: 0.9009     \n",
      "Epoch 4/5\n",
      "32951/32951 [==============================] - 2s - loss: 0.0757 - acc: 0.9009     \n",
      "Epoch 5/5\n",
      "32951/32951 [==============================] - 1s - loss: 0.0751 - acc: 0.9021     \n"
     ]
    }
   ],
   "source": [
    "for train_index, cross_val_index in kf:\n",
    "    \n",
    "    train, cross_val = Data.iloc[train_index], Data.iloc[cross_val_index]\n",
    "    train_Y=train['y']\n",
    "    train_X=train.drop(['y'],axis=1)\n",
    "    cross_val_Y=cross_val['y']\n",
    "    cross_val_X=cross_val.drop(['y'],axis=1)\n",
    "    predict=list()\n",
    "     \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"error\"\n",
    "    param['eta'] = 0.3\n",
    "    param['gamma'] = 0\n",
    "    param['max_depth'] = 6\n",
    "    param['min_child_weight']=1\n",
    "    param['max_delta_step'] = 0\n",
    "    param['subsample']= 1\n",
    "    param['colsample_bytree']=1\n",
    "    param['silent'] = 1\n",
    "    param['seed'] = 0\n",
    "    param['base_score'] = 0.5\n",
    "    dtrain = xgb.DMatrix(train_X,label=train_Y)\n",
    "    dcross_val = xgb.DMatrix(cross_val_X,label=cross_val_Y)\n",
    "    XGB = xgb.train(param, dtrain)\n",
    "    predict=XGB.predict(dcross_val)\n",
    "    predict[(predict>=0.5)]=1\n",
    "    predict[(predict<0.5)]=0\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_XGB.append((np.mean((predict - cross_val_Y) ** 2)))\n",
    "    \n",
    "\n",
    "    #Gradient Boosting (Sklearn)\n",
    "    GB = GradientBoostingClassifier()\n",
    "    GB.fit(train_X,train_Y)\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_GB.append(np.mean((GB.predict(cross_val_X) - cross_val_Y) ** 2))\n",
    "    #Explained variance score: 1 is perfect prediction (Cross Validation Data)\n",
    "    variance_GB.append(GB.score(cross_val_X, cross_val_Y))\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    MLP = Sequential()\n",
    "    #Building the model\n",
    "    MLP.add(Dense(output_dim=64, input_dim=20, init='uniform', activation='sigmoid'))\n",
    "    MLP.add(Dense(output_dim=1, input_dim=64,activation='sigmoid'))\n",
    "    MLP.compile(optimizer='rmsprop',loss='mean_squared_error',metrics=['accuracy'])\n",
    "    MLP.fit(train_X.as_matrix(), train_Y.as_matrix(), nb_epoch=5, batch_size=32)\n",
    "    #The mean square error and accuracy on cross validation data.\n",
    "    eval=MLP.test_on_batch(cross_val_X.as_matrix(), cross_val_Y.as_matrix(), sample_weight=None)\n",
    "    mean_MLP.append(eval[0])\n",
    "    variance_MLP.append(eval[1])\n",
    "    \n",
    "    #Decision Tree\n",
    "    DT = DecisionTreeClassifier()\n",
    "    DT.fit(train_X,train_Y)\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_DT.append(np.mean((DT.predict(cross_val_X) - cross_val_Y) ** 2))\n",
    "    #Explained variance score: 1 is perfect prediction (Cross Validation Data)\n",
    "    variance_DT.append(DT.score(cross_val_X, cross_val_Y))\n",
    "    \n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(train_X,train_Y)\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_RF.append(np.mean((RF.predict(cross_val_X) - cross_val_Y) ** 2))\n",
    "    #Explained variance score: 1 is perfect prediction (Cross Validation Data)\n",
    "    variance_RF.append(RF.score(cross_val_X, cross_val_Y))\n",
    "    \n",
    "    #Scaling the data\n",
    "    train_X=preprocessing.StandardScaler().fit_transform(train_X)\n",
    "    cross_val_X=preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    \n",
    "    #Linear Regression\n",
    "    predict=list()\n",
    "    LR = linear_model.LinearRegression()\n",
    "    LR.fit(train_X,train_Y)\n",
    "    predict=LR.predict(cross_val_X)\n",
    "    predict[(predict>=0.5)]=1\n",
    "    predict[(predict<0.5)]=0\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_LinearRegression.append((np.mean((predict - cross_val_Y) ** 2)))\n",
    "    \n",
    "    #Logistic Regression (Default=L2)\n",
    "    L2 = linear_model.LogisticRegression(penalty='l2')\n",
    "    L2.fit(train_X,train_Y)\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_L2.append((np.mean((L2.predict(cross_val_X) - cross_val_Y) ** 2)))\n",
    "    #Explained variance score: 1 is perfect prediction (Cross Validation Data)\n",
    "    variance_L2.append(L2.score(cross_val_X, cross_val_Y))\n",
    "    \n",
    "    #Logistic Regression-L1\n",
    "    L1 = linear_model.LogisticRegression(penalty='l1')\n",
    "    L1.fit(train_X,train_Y)\n",
    "    #The mean square error (Cross Validation Data)\n",
    "    mean_L1.append(np.mean((L1.predict(cross_val_X) - cross_val_Y) ** 2))\n",
    "    #Explained variance score: 1 is perfect prediction (Cross Validation Data)\n",
    "    variance_L1.append(L1.score(cross_val_X, cross_val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "avg_LinearRegression=np.mean(mean_LinearRegression)\n",
    "avg_L2=np.mean(mean_L2)\n",
    "avg_L1=np.mean(mean_L1)\n",
    "avg_DT=np.mean(mean_DT)\n",
    "avg_RF=np.mean(mean_RF)\n",
    "avg_MLP=np.mean(mean_MLP)\n",
    "avg_GB=np.mean(mean_GB)\n",
    "avg_ET=np.mean(mean_ET)\n",
    "avg_XGB=np.mean(mean_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean Error (Linear Regression)\n",
      " 0.0935709651348\n",
      " Mean Error (Logistic Regression - L2)\n",
      " 0.0897106722687\n",
      " Mean Error (Logistic Regression - L1)\n",
      " 0.089734950006\n",
      " Mean Error (Decision Tree)\n",
      " 0.110978765955\n",
      " Mean Error (Random Forest)\n",
      " 0.0903903575441\n",
      " Mean Error (Multi Layer Perceptron)\n",
      " 0.0755187\n",
      " Mean Error (Gradient Boosting - Sklearn)\n",
      " 0.0832523903491\n",
      " Mean Error (Gradient Boosting - XGBoost)\n",
      " 0.0835922668819\n"
     ]
    }
   ],
   "source": [
    "print (' Mean Error (Linear Regression)\\n',avg_LinearRegression)\n",
    "print (' Mean Error (Logistic Regression - L2)\\n',avg_L2)\n",
    "print (' Mean Error (Logistic Regression - L1)\\n',avg_L1)\n",
    "print (' Mean Error (Decision Tree)\\n',avg_DT)\n",
    "print (' Mean Error (Random Forest)\\n',avg_RF)\n",
    "print (' Mean Error (Multi Layer Perceptron)\\n',avg_MLP)\n",
    "print (' Mean Error (Gradient Boosting - Sklearn)\\n',avg_GB)\n",
    "print (' Mean Error (Gradient Boosting - XGBoost)\\n',avg_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
