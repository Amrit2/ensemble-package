{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading the data, into a Data Frame.\n",
    "Data = pd.read_csv('/home/prajwal/Desktop/bank-additional/bank-additional-full.csv',delimiter=';',header=0)\n",
    "\n",
    "#Encoding the data, encoding the string values into numerical values.\n",
    "encode = preprocessing.LabelEncoder()\n",
    "\n",
    "#Selcting the columns of string data type\n",
    "names = Data.select_dtypes(include=['object'])\n",
    "\n",
    "#Function that encodes the string values to numerical values.\n",
    "def enc(data,column):\n",
    "    data[column] = encode.fit_transform(data[column])\n",
    "    return data\n",
    "for column in names:\n",
    "        Data = enc(Data,column)\n",
    "        \n",
    "#Splitting the data into training and testing datasets (Stratified Split)\n",
    "Data, test = train_test_split(Data, test_size = 0.1,stratify=Data['y'])\n",
    "\n",
    "#The list of base models.\n",
    "base_model_list=['XGBoost','Multi Layer Perceptron','Decision Tree','Random Forest','Linear Regression','L1','L2']\n",
    "\n",
    "#The list of second level models.\n",
    "second_level_model_list=['Stack','Blend']\n",
    "\n",
    "model_name_list=list() #A list that contains both the models and the model names. Will be used during cross validation.\n",
    "second_level_model_name_list=list()\n",
    "\n",
    "#Initializing two data frames that will be used as training data for the stacked model.\n",
    "stack_X = pd.DataFrame() #The data frame will contain the predictions of the base models.\n",
    "stack_Y = pd.DataFrame() #The data frame will contain the calss labels of the base models.\n",
    "\n",
    "#Initializing two data frames that will be used as training data for the blending model.\n",
    "blend_X = pd.DataFrame() #The data frames will contain the predictions and raw features  of the base models.\n",
    "raw_features_X = pd.DataFrame() #The data frames will contain the raw features  of the data, which will be concatenated with the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used to convert the predictions of the base models into a DataFrame.\n",
    "def build_data_frame(data):\n",
    "    data_frame = pd.DataFrame(data).T\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the parameters for the XGBoost (Gradient Boosting) Algorithm.\n",
    "def param_set():\n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = 0.3\n",
    "    param['gamma'] = 0\n",
    "    param['max_depth'] = 6\n",
    "    param['min_child_weight'] = 1\n",
    "    param['max_delta_step'] = 0\n",
    "    param['subsample'] = 1\n",
    "    param['colsample_bytree'] = 1\n",
    "    param['silent'] = 1\n",
    "    param['seed'] = 0\n",
    "    param['base_score'] = 0.5\n",
    "    param['lambda_bias'] = 1\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function is used to train the base and stacking models. Returns all the models to be used for further computations.\n",
    "def train_base_models(train_X,train_Y,model):\n",
    "    \n",
    "    param = param_set()\n",
    "    \n",
    "    if(model=='XGBoost'):\n",
    "        \n",
    "        #Gradient Boosting\n",
    "        dtrain = xgb.DMatrix(train_X,label=train_Y)\n",
    "        gradient_boosting = xgb.train(param, dtrain)\n",
    "        return gradient_boosting\n",
    "       \n",
    "    elif(model=='Multi Layer Perceptron'):\n",
    "        \n",
    "        #Multi Layer Perceptron\n",
    "        multi_layer_perceptron = Sequential()\n",
    "        multi_layer_perceptron.add(Dense(output_dim = 64, input_dim = 20, init = 'uniform', activation = 'sigmoid'))\n",
    "        multi_layer_perceptron.add(Dense(output_dim = 1, input_dim = 64,activation = 'sigmoid',))\n",
    "        multi_layer_perceptron.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "        multi_layer_perceptron.fit(train_X.as_matrix(), train_Y.as_matrix(), nb_epoch = 5, batch_size = 128)\n",
    "        return multi_layer_perceptron\n",
    "        \n",
    "    elif(model=='Decision Tree'):\n",
    "        #Decision Tree\n",
    "        decision_tree = DecisionTreeClassifier(max_depth = 6)\n",
    "        decision_tree.fit(train_X,train_Y)\n",
    "        return decision_tree\n",
    "        \n",
    "    elif(model=='Random Forest'):\n",
    "        \n",
    "        #Random Forest (Deafult=10 Trees)\n",
    "        random_forest = RandomForestClassifier()\n",
    "        random_forest.fit(train_X,train_Y)\n",
    "        return random_forest\n",
    "        \n",
    "    elif(model=='Linear Regression'):\n",
    "        \n",
    "        #Scaling the data\n",
    "        train_X = preprocessing.StandardScaler().fit_transform(train_X)\n",
    "        \n",
    "        #Linear Regression\n",
    "        linear_regression = linear_model.LinearRegression()\n",
    "        linear_regression.fit(train_X,train_Y)\n",
    "        return linear_regression\n",
    "        \n",
    "    elif(model=='L1'):\n",
    "        \n",
    "        #Scaling the data\n",
    "        train_X = preprocessing.StandardScaler().fit_transform(train_X)\n",
    "        \n",
    "        #Logistic Regression (L1)\n",
    "        logistic_regression_L1 = linear_model.LogisticRegression(penalty = 'l1')\n",
    "        logistic_regression_L1.fit(train_X,train_Y)\n",
    "        return logistic_regression_L1\n",
    "        \n",
    "    elif(model=='L2'):\n",
    "       \n",
    "        #Scaling the data\n",
    "        train_X = preprocessing.StandardScaler().fit_transform(train_X)\n",
    "        \n",
    "        #Logistic Regression (L2)\n",
    "        logistic_regression_L2 = linear_model.LogisticRegression(penalty = 'l2')\n",
    "        logistic_regression_L2.fit(train_X,train_Y)\n",
    "        return logistic_regression_L2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_second_level_models(train_X,train_Y,model):\n",
    "    \n",
    "    param = param_set()\n",
    "    \n",
    "    #Trains the stacking model (Gradient Boosting - XGBoost)\n",
    "    if(model == 'Stack'):\n",
    "        \n",
    "        dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "        stack = xgb.train(param, dtrain)\n",
    "        return stack\n",
    "    \n",
    "    #Trains the blending model (Gradient Boosting - XGBoost)\n",
    "    elif(model == 'Blend'):\n",
    "        \n",
    "        dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "        blend = xgb.train(param, dtrain)\n",
    "        return blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_stack_model(train_X,train_Y):\n",
    "    \n",
    "    param = param_set()\n",
    "    #Trains the Stacking model (Gradient Boosting - XGBoost)\n",
    "    dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "    stack = xgb.train(param, dtrain)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_blend_model(train_X,train_Y): \n",
    "    \n",
    "    param = param_set()\n",
    "    #Trains the blending model (Gradient Boosting - XGBoost)\n",
    "    dtrain = xgb.DMatrix(train_X,label = train_Y)\n",
    "    blend = xgb.train(param, dtrain)\n",
    "    return blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function calculates area under the curve and predictions on the given data, for the specified model.\n",
    "def cross_validation(model_name,model,cross_val_X,cross_val_Y):\n",
    "    \n",
    "    if(model_name == 'XGBoost'): \n",
    "        \n",
    "        predict = model.predict(xgb.DMatrix(cross_val_X,label=cross_val_Y))\n",
    "        \n",
    "    elif(model_name == 'Linear Regression'):\n",
    "        \n",
    "        cross_val_X=preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "        predict = model.predict(cross_val_X)\n",
    "        \n",
    "    elif(model_name == 'L1' or model_name == 'L2'):\n",
    "        \n",
    "        cross_val_X=preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "        predict = model.predict_proba(cross_val_X)[:,1]\n",
    "        \n",
    "    elif(model_name == 'Multi Layer Perceptron'):\n",
    "        \n",
    "        predict = model.predict_on_batch(cross_val_X)\n",
    "    else:\n",
    "        \n",
    "        predict = model.predict_proba(cross_val_X)[:,1]\n",
    "        \n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    \n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialzing the variables that will be used to calculate the area under the curve. (cross Validation Data)\n",
    "metric_linear_regression=list()\n",
    "avg_linear_regeression=0\n",
    "metric_logistic_regression_L2=list()\n",
    "avg_logistic_regression_L2=0\n",
    "metric_logistic_regression_L1=list()\n",
    "avg_logistic_regression_L1=0\n",
    "metric_decision_tree=list()\n",
    "avg_decision_tree=0\n",
    "metric_random_forest=list()\n",
    "avg_random_forest=0\n",
    "metric_multi_layer_perceptron=list()\n",
    "avg_multi_layer_perceptron=0\n",
    "metric_gradient_boosting=list()\n",
    "avg_gradient_boosting=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cross Validation using Stratified K Fold\n",
    "train, cross_val = train_test_split(Data, test_size = 0.5,stratify=Data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.3492 - acc: 0.8778     \n",
      "Epoch 2/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.3002 - acc: 0.8873     \n",
      "Epoch 3/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.2785 - acc: 0.8873     \n",
      "Epoch 4/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.2695 - acc: 0.8873     \n",
      "Epoch 5/5\n",
      "18534/18534 [==============================] - 0s - loss: 0.2634 - acc: 0.8885     \n"
     ]
    }
   ],
   "source": [
    "#Training the base models, and calculating AUC on the cross validation data.\n",
    "    \n",
    "#Selecting the data (Traing Data & Cross Validation Data)\n",
    "train_Y=train['y']\n",
    "train_X=train.drop(['y'],axis=1)\n",
    "cross_val_Y=cross_val['y']\n",
    "cross_val_X=cross_val.drop(['y'],axis=1)\n",
    "    \n",
    "#Training the base models parallely, the resulting models are stored in the variable models in the form of a list.\n",
    "models=(Parallel(n_jobs=-1)(delayed(train_base_models)(train_X, train_Y, model) for model in base_model_list))\n",
    "    \n",
    "#The list will contain both the models and the model names. The list is used in the next step.\n",
    "for i in range(len(base_model_list)):\n",
    "    model_name_list.append((base_model_list[i],models[i]))\n",
    "        \n",
    "#Computing the AUC and Predictions of all the base models on the cross validation data parallely.\n",
    "auc_predict_cross_val=(Parallel(n_jobs=-1)(delayed(cross_validation)(model_name,model,cross_val_X,cross_val_Y) for model_name,model in model_name_list))\n",
    "    \n",
    "#Gradient Boosting (XGBoost)\n",
    "#The AUC error (Cross Validation Data)\n",
    "auc,predict_gradient_boosting=auc_predict_cross_val[0][0],auc_predict_cross_val[0][1]\n",
    "metric_gradient_boosting.append(auc)\n",
    "    \n",
    "#Multi Layer Perceptron\n",
    "#The AUC (Cross Validation Data)\n",
    "predict_mlp=list()\n",
    "auc,predict_multi_layer_perceptron=auc_predict_cross_val[1][0],auc_predict_cross_val[1][1]\n",
    "metric_multi_layer_perceptron.append(auc)\n",
    "    \n",
    "#predict_multi_layer_perceptron returns a list of lists containing the predictions, this cannot be converted to a dataframe.\n",
    "#This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "for i in predict_multi_layer_perceptron:\n",
    "    predict_mlp.append(float(i))\n",
    "    \n",
    "#Decision Tree)\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_decision_tree=auc_predict_cross_val[2][0],auc_predict_cross_val[2][1]\n",
    "metric_decision_tree.append(auc)\n",
    "    \n",
    "#Random Forest (Deafult=10 Trees)\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_random_forest=auc_predict_cross_val[3][0],auc_predict_cross_val[3][1]\n",
    "metric_random_forest.append(auc)\n",
    "    \n",
    "#Linear Regression\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_linear_regression=auc_predict_cross_val[4][0],auc_predict_cross_val[4][1]\n",
    "metric_linear_regression.append(auc)\n",
    "    \n",
    "#Logistic Regression (Default=l2)\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_logistic_regression_L1=auc_predict_cross_val[5][0],auc_predict_cross_val[5][1]\n",
    "metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    \n",
    "#Logistic Regression-L2\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_logistic_regression_L2=auc_predict_cross_val[6][0],auc_predict_cross_val[6][1]\n",
    "metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "#Building a list that contains all the predictions of the base models.\n",
    "predict_list=[predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    \n",
    "#Converting the above list of predictions into a dataframe, which will be used to train the stacking model.\n",
    "stack_X=stack_X.append(build_data_frame(predict_list))\n",
    "    \n",
    "#Building a list that contains all the raw features, used as cross validation data for the base models.\n",
    "raw_features_X=raw_features_X.append(cross_val_X,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating the average AUC across all the AUC computed on the cross validation folds.\n",
    "avg_linear_regression=np.mean(metric_linear_regression)\n",
    "avg_logistic_regression_L2=np.mean(metric_logistic_regression_L2)\n",
    "avg_logistic_regression_L1=np.mean(metric_logistic_regression_L1)\n",
    "avg_decision_tree=np.mean(metric_decision_tree)\n",
    "avg_random_forest=np.mean(metric_random_forest)\n",
    "avg_multi_layer_perceptron=np.mean(metric_multi_layer_perceptron)\n",
    "avg_gradient_boosting=np.mean(metric_gradient_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC (Linear Regression)\n",
      " 0.925996006678\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.927985882669\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.928021189974\n",
      " AUC (Decision Tree)\n",
      " 0.92407808479\n",
      " AUC (Random Forest)\n",
      " 0.916565811534\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.839432091401\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.943930501131\n"
     ]
    }
   ],
   "source": [
    "#Printing the AUC for the base models.\n",
    "print (' AUC (Linear Regression)\\n',avg_linear_regression)\n",
    "print (' AUC (Logistic Regression - L2)\\n',avg_logistic_regression_L2)\n",
    "print (' AUC (Logistic Regression - L1)\\n',avg_logistic_regression_L1)\n",
    "print (' AUC (Decision Tree)\\n',avg_decision_tree)\n",
    "print (' AUC (Random Forest)\\n',avg_random_forest)\n",
    "print (' AUC (Multi Layer Perceptron)\\n',avg_multi_layer_perceptron)\n",
    "print (' AUC (Gradient Boosting - XGBoost)\\n',avg_gradient_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converting the above list of predictions and raw features (Concatenate) into a dataframe, which will be used to train the blending model.\n",
    "blend_X=pd.concat([raw_features_X, stack_X], axis=1,ignore_index=True)\n",
    "\n",
    "#Training the Stacking and Blending models parallely using the predictions of base models on the cross validation data.\n",
    "model_second_level = Parallel(n_jobs=-1)(delayed(model_function)(train_X,train_Y)for model_function,train_X,train_Y in [(train_stack_model,stack_X,cross_val_Y),(train_blend_model,blend_X,cross_val_Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialzing the variables that will be used to calculate the area under the curve. (Test Data)\n",
    "metric_linear_regression=list()\n",
    "metric_logistic_regression_L2=list()\n",
    "metric_logistic_regression_L1=list()\n",
    "metric_decision_tree=list()\n",
    "metric_random_forest=list()\n",
    "metric_multi_layer_perceptron=list()\n",
    "metric_gradient_boosting=list()\n",
    "metric_stack=list()\n",
    "metric_blend=list()\n",
    "blend_X = pd.DataFrame()\n",
    "raw_features_X = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating AUC for all the models (Base Models & Stack Model) on the test data.\n",
    "\n",
    "#Selecting the test data\n",
    "test_Y=test['y']\n",
    "test_X=test.drop(['y'],axis=1)\n",
    "\n",
    "#Computing the AUC and Predictions of all the base models on the test data parallely.\n",
    "auc_predict_test_base=(Parallel(n_jobs=-1)(delayed(cross_validation)(model_name,model,test_X,test_Y) for model_name,model in model_name_list))\n",
    "    \n",
    "#Gradient Boosting (XGBoost)\n",
    "#The AUC error (Cross Validation Data)\n",
    "auc,predict_gradient_boosting=auc_predict_test_base[0][0],auc_predict_test_base[0][1]\n",
    "metric_gradient_boosting.append(auc)\n",
    "    \n",
    "#Multi Layer Perceptron\n",
    "#The AUC (Cross Validation Data)\n",
    "predict_mlp=list()\n",
    "auc,predict_multi_layer_perceptron=auc_predict_test_base[1][0],auc_predict_test_base[1][1]\n",
    "metric_multi_layer_perceptron.append(auc)\n",
    "#predict_multi_layer_perceptron returns a list of lists containing the predictions, this cannot be converted to a dataframe.\n",
    "#This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "for i in predict_multi_layer_perceptron:\n",
    "    predict_mlp.append(float(i))\n",
    "    \n",
    "#Decision Tree\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_decision_tree=auc_predict_test_base[2][0],auc_predict_test_base[2][1]\n",
    "metric_decision_tree.append(auc)\n",
    "    \n",
    "#Random Forest (Deafult=10 Trees)\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_random_forest=auc_predict_test_base[3][0],auc_predict_test_base[3][1]\n",
    "metric_random_forest.append(auc)\n",
    "    \n",
    "#Linear Regression\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_linear_regression=auc_predict_test_base[4][0],auc_predict_test_base[4][1]\n",
    "metric_linear_regression.append(auc)\n",
    "    \n",
    "#Logistic Regression (Default=l2)\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_logistic_regression_L1=auc_predict_test_base[5][0],auc_predict_test_base[5][1]\n",
    "metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    \n",
    "#Logistic Regression-L2\n",
    "#The AUC (Cross Validation Data)\n",
    "auc,predict_logistic_regression_L2=auc_predict_test_base[6][0],auc_predict_test_base[6][1]\n",
    "metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "#Building a list that contains all the predictions of the base models.\n",
    "predict_list=[predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    \n",
    "\n",
    "\n",
    "dstack_X=build_data_frame(predict_list)#Converting the list of predictions into a dataframe.\n",
    "raw_features_X=raw_features_X.append(test_X,ignore_index=True)\n",
    "blend_X=pd.concat([raw_features_X, dstack_X], axis=1,ignore_index=True)#Converting the above list of predictions and raw features (Concatenate) into a dataframe\n",
    "\n",
    "#Computing the AUC and Predictions of the Stacking and Blending models on the test data parallely.\n",
    "auc_predict_test_second_level=Parallel(n_jobs=-1)(delayed(cross_validation)('XGBoost',model,test_X, test_Y) for test_X,model in ((dstack_X,model_second_level[0]),(blend_X,model_second_level[1])))\n",
    "\n",
    "#Stacking (XGBoost - Gradient Boosting)\n",
    "auc,predict_stack=auc_predict_test_second_level[0][0],auc_predict_test_second_level[0][1]\n",
    "metric_stack=(auc)    \n",
    "\n",
    "#Blending (XGBoost - Gradient Boosting)\n",
    "auc,predict_blend=auc_predict_test_second_level[1][0],auc_predict_test_second_level[1][1]\n",
    "metric_blend=(auc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC (Linear Regression)\n",
      " [0.93057543987923963]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.93294082268031508]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.93299919807538101]\n",
      " AUC (Decision Tree)\n",
      " [0.92961637341384029]\n",
      " AUC (Random Forest)\n",
      " [0.91489280154724273]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.83724232275107313]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94839998348978727]\n",
      " AUC (Stacking)\n",
      " 0.949621149583\n",
      " AUC (Blending)\n",
      " 0.951237086655\n"
     ]
    }
   ],
   "source": [
    "print (' AUC (Linear Regression)\\n',metric_linear_regression)\n",
    "print (' AUC (Logistic Regression - L2)\\n',metric_logistic_regression_L2)\n",
    "print (' AUC (Logistic Regression - L1)\\n',metric_logistic_regression_L1)\n",
    "print (' AUC (Decision Tree)\\n',metric_decision_tree)\n",
    "print (' AUC (Random Forest)\\n',metric_random_forest)\n",
    "print (' AUC (Multi Layer Perceptron)\\n',metric_multi_layer_perceptron)\n",
    "print (' AUC (Gradient Boosting - XGBoost)\\n',metric_gradient_boosting)\n",
    "print (' AUC (Stacking)\\n',metric_stack)\n",
    "print (' AUC (Blending)\\n',metric_blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
