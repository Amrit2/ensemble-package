{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "Data = pd.read_csv('/home/prajwal/Desktop/bank-additional/bank-additional-full.csv',delimiter=';',header=0)\n",
    "\n",
    "#Encoding the data, encoding the string values into numerical values\n",
    "encode = preprocessing.LabelEncoder()\n",
    "\n",
    "#Selcting the columns of string data type\n",
    "names=Data.select_dtypes(include=['object'])\n",
    "\n",
    "#Function that encodes the string values to numerical values\n",
    "def enc(data,column):\n",
    "    data[column] = encode.fit_transform(data[column])\n",
    "    return data\n",
    "for column in names:\n",
    "        Data=enc(Data,column)\n",
    "\n",
    "Data, test = train_test_split(Data, test_size = 0.1)\n",
    "#Data.job = encode.fit_transform(Data.job)\n",
    "#Data.marital = encode.fit_transform(Data.marital)\n",
    "#Data.education = encode.fit_transform(Data.education)\n",
    "#Data.default = encode.fit_transform(Data.default)\n",
    "#Data.housing = encode.fit_transform(Data.housing)\n",
    "#Data.loan = encode.fit_transform(Data.loan)\n",
    "#Data.contact = encode.fit_transform(Data.contact)\n",
    "#Data.month = encode.fit_transform(Data.month)\n",
    "#Data.day_of_week = encode.fit_transform(Data.day_of_week)\n",
    "#Data.poutcome = encode.fit_transform(Data.poutcome)\n",
    "#Data.y = encode.fit_transform(Data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_frame(data):\n",
    "    data_frame=pd.DataFrame(data).T\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_set():\n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = 0.3\n",
    "    param['gamma'] = 0\n",
    "    param['max_depth'] = 6\n",
    "    param['min_child_weight']=1\n",
    "    param['max_delta_step'] = 0\n",
    "    param['subsample']= 1\n",
    "    param['colsample_bytree']=1\n",
    "    param['silent'] = 1\n",
    "    param['seed'] = 0\n",
    "    param['base_score'] = 0.5\n",
    "    param['lambda_bias']=1\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_models(train_X,train_Y,model):\n",
    "    param = param_set()\n",
    "    if(model=='base'):\n",
    "        #Gradient Boosting\n",
    "        dtrain = xgb.DMatrix(train_X,label=train_Y)\n",
    "        gradient_boosting = xgb.train(param, dtrain)\n",
    "        \n",
    "        #Multi Layer Perceptron\n",
    "        multi_layer_perceptron = Sequential()\n",
    "        multi_layer_perceptron.add(Dense(output_dim=64, input_dim=20, init='uniform', activation='sigmoid'))\n",
    "        multi_layer_perceptron.add(Dense(output_dim=1, input_dim=64,activation='sigmoid',))\n",
    "        multi_layer_perceptron.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        multi_layer_perceptron.fit(train_X.as_matrix(), train_Y.as_matrix(), nb_epoch=5, batch_size=128)\n",
    "        \n",
    "        #Decision Tree\n",
    "        decision_tree = DecisionTreeClassifier(max_depth=6)\n",
    "        decision_tree.fit(train_X,train_Y)\n",
    "        \n",
    "        #Random Forest (Deafult=10 Trees)\n",
    "        random_forest = RandomForestClassifier()\n",
    "        random_forest.fit(train_X,train_Y)\n",
    "        \n",
    "        #Scaling the data\n",
    "        train_X=preprocessing.StandardScaler().fit_transform(train_X) \n",
    "        \n",
    "        #Linear Regression\n",
    "        predict=list()\n",
    "        linear_regression = linear_model.LinearRegression()\n",
    "        linear_regression.fit(train_X,train_Y)\n",
    "        #Logistic Regression (L1)\n",
    "        \n",
    "        logistic_regression_L1 = linear_model.LogisticRegression(penalty='l1')\n",
    "        logistic_regression_L1.fit(train_X,train_Y)\n",
    "        #Logistic Regression (L2)\n",
    "        \n",
    "        logistic_regression_L2 = linear_model.LogisticRegression(penalty='l2')\n",
    "        logistic_regression_L2.fit(train_X,train_Y)\n",
    "        \n",
    "        return {'XGBoost':gradient_boosting,'Multi Layer Perceptron':multi_layer_perceptron,'Decision Tree':decision_tree,\n",
    "           'Random Forest':random_forest,'Linear Regression':linear_regression,'L1':logistic_regression_L1,\n",
    "            'L2':logistic_regression_L2}\n",
    "    else:\n",
    "        dtrain = xgb.DMatrix(train_X,label=train_Y)\n",
    "        stack = xgb.train(param, dtrain)\n",
    "        return {'Stack':stack}\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(model_name,model,cross_val_X,cross_val_Y):\n",
    "    if(model_name=='Gradient Boosting' or model_name=='Linear Regression'):\n",
    "        predict=model.predict(cross_val_X)\n",
    "        \n",
    "    elif(model_name=='Multi Layer Perceptron'):\n",
    "        predict=model.predict_on_batch(cross_val_X)\n",
    "    else:\n",
    "        predict=model.predict_proba(cross_val_X)[:,1]\n",
    "    auc=roc_auc_score(cross_val_Y,predict)\n",
    "    return[auc,predict]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric_linear_regression=list()\n",
    "avg_linear_regeression=0\n",
    "metric_logistic_regression_L2=list()\n",
    "avg_logistic_regression_L2=0\n",
    "metric_logistic_regression_L1=list()\n",
    "avg_logistic_regression_L1=0\n",
    "metric_decision_tree=list()\n",
    "avg_decision_tree=0\n",
    "metric_random_forest=list()\n",
    "avg_random_forest=0\n",
    "metric_multi_layer_perceptron=list()\n",
    "avg_multi_layer_perceptron=0\n",
    "metric_gradient_boosting=list()\n",
    "avg_gradient_boosting=0\n",
    "metric_XGB=list()\n",
    "avg_XGB=0\n",
    "metric_stack=list()\n",
    "avg_stack=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cross Validation using Stratified K Fold\n",
    "kf = StratifiedKFold(Data['y'], n_folds=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29654/29654 [==============================] - 0s - loss: 0.2982 - acc: 0.8865     \n",
      "Epoch 2/5\n",
      "29654/29654 [==============================] - 0s - loss: 0.2681 - acc: 0.8927     \n",
      "Epoch 3/5\n",
      "29654/29654 [==============================] - 0s - loss: 0.2585 - acc: 0.8989     \n",
      "Epoch 4/5\n",
      "29654/29654 [==============================] - 0s - loss: 0.2556 - acc: 0.9035     \n",
      "Epoch 5/5\n",
      "29654/29654 [==============================] - 0s - loss: 0.2517 - acc: 0.9022     \n",
      "Epoch 1/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.3132 - acc: 0.8864     \n",
      "Epoch 2/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2803 - acc: 0.8864     \n",
      "Epoch 3/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2685 - acc: 0.8886     \n",
      "Epoch 4/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2610 - acc: 0.8932     \n",
      "Epoch 5/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2551 - acc: 0.8999     \n",
      "Epoch 1/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.3958 - acc: 0.8218     \n",
      "Epoch 2/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2684 - acc: 0.8900     \n",
      "Epoch 3/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2569 - acc: 0.8977     \n",
      "Epoch 4/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2538 - acc: 0.9011     \n",
      "Epoch 5/5\n",
      "29655/29655 [==============================] - 0s - loss: 0.2491 - acc: 0.9030     \n",
      "Epoch 1/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.3710 - acc: 0.8444     \n",
      "Epoch 2/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2718 - acc: 0.8869     \n",
      "Epoch 3/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2585 - acc: 0.8950     \n",
      "Epoch 4/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2537 - acc: 0.9014     \n",
      "Epoch 5/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2496 - acc: 0.9038     \n",
      "Epoch 1/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.3382 - acc: 0.8672     \n",
      "Epoch 2/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2691 - acc: 0.8873     \n",
      "Epoch 3/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2599 - acc: 0.8965     \n",
      "Epoch 4/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2571 - acc: 0.8990     \n",
      "Epoch 5/5\n",
      "29656/29656 [==============================] - 0s - loss: 0.2541 - acc: 0.9004     \n"
     ]
    }
   ],
   "source": [
    "for train_index, cross_val_index in kf:\n",
    "    \n",
    "    #Selecting the data\n",
    "    train, cross_val = Data.iloc[train_index], Data.iloc[cross_val_index]\n",
    "    train_Y=train['y']\n",
    "    train_X=train.drop(['y'],axis=1)\n",
    "    cross_val_Y=cross_val['y']\n",
    "    cross_val_X=cross_val.drop(['y'],axis=1)\n",
    "    \n",
    "    model=train_models(train_X,train_Y,'base')\n",
    "  \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Cross Validation Data)\n",
    "    [auc,predict_XGB]=cross_validation('Gradient Boosting',model['XGBoost'],xgb.DMatrix(cross_val_X,label=cross_val_Y),cross_val_Y)\n",
    "    metric_XGB.append(auc)\n",
    "\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_multi_layer_perceptron]=cross_validation('Multi Layer Perceptron',model['Multi Layer Perceptron'],cross_val_X,cross_val_Y)\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "\n",
    "\n",
    "    #Decision Tree)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_decision_tree]=cross_validation('Decision Tree',model['Decision Tree'],cross_val_X,cross_val_Y)\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_random_forest]=random_forest(train_X,train_Y,cross_val_X,cross_val_Y)\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    cross_val_X=preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    #Linear Regression\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_linear_regression]=cross_validation('Linear Regression',model['Linear Regression'],cross_val_X,cross_val_Y)\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_logistic_regression_L2]=cross_validation('L2',model['L2'],cross_val_X,cross_val_Y)\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L1\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_logistic_regression_L1]=cross_validation('L1',model['L1'],cross_val_X,cross_val_Y)\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    predict_list=[predict_XGB,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1]\n",
    "    \n",
    "    #Stacking (XGBoost - Gradient Boosting)\n",
    "    stack_X=build_data_frame(predict_list)\n",
    "    model_stack=train_models(stack_X,cross_val_Y,'stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_linear_regression=np.mean(metric_linear_regression)\n",
    "avg_logistic_regression_L2=np.mean(metric_logistic_regression_L2)\n",
    "avg_logistic_regression_L1=np.mean(metric_logistic_regression_L1)\n",
    "avg_decision_tree=np.mean(metric_decision_tree)\n",
    "avg_random_forest=np.mean(metric_random_forest)\n",
    "avg_multi_layer_perceptron=np.mean(metric_multi_layer_perceptron)\n",
    "avg_XGB=np.mean(metric_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC (Linear Regression)\n",
      " 0.927891666682\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.929192288875\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.92920648888\n",
      " AUC (Decision Tree)\n",
      " 0.924981840236\n",
      " AUC (Random Forest)\n",
      " 0.918014870312\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.876715475773\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.947727495764\n"
     ]
    }
   ],
   "source": [
    "print (' AUC (Linear Regression)\\n',avg_linear_regression)\n",
    "print (' AUC (Logistic Regression - L2)\\n',avg_logistic_regression_L2)\n",
    "print (' AUC (Logistic Regression - L1)\\n',avg_logistic_regression_L1)\n",
    "print (' AUC (Decision Tree)\\n',avg_decision_tree)\n",
    "print (' AUC (Random Forest)\\n',avg_random_forest)\n",
    "print (' AUC (Multi Layer Perceptron)\\n',avg_multi_layer_perceptron)\n",
    "print (' AUC (Gradient Boosting - XGBoost)\\n',avg_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_linear_regression=list()\n",
    "avg_linear_regeression=0\n",
    "metric_logistic_regression_L2=list()\n",
    "avg_logistic_regression_L2=0\n",
    "metric_logistic_regression_L1=list()\n",
    "avg_logistic_regression_L1=0\n",
    "metric_decision_tree=list()\n",
    "avg_decision_tree=0\n",
    "metric_random_forest=list()\n",
    "avg_random_forest=0\n",
    "metric_multi_layer_perceptron=list()\n",
    "avg_multi_layer_perceptron=0\n",
    "metric_gradient_boosting=list()\n",
    "avg_gradient_boosting=0\n",
    "metric_XGB=list()\n",
    "avg_XGB=0\n",
    "metric_stack=list()\n",
    "avg_stack=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [1]:\n",
    "    \n",
    "    cross_val_Y=test['y']\n",
    "    cross_val_X=test.drop(['y'],axis=1)\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Cross Validation Data)\n",
    "    [auc,predict_XGB]=cross_validation('Gradient Boosting',model['XGBoost'],xgb.DMatrix(cross_val_X,label=cross_val_Y),cross_val_Y)\n",
    "    metric_XGB.append(auc)\n",
    "\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_multi_layer_perceptron]=cross_validation('Multi Layer Perceptron',model['Multi Layer Perceptron'],cross_val_X,cross_val_Y)\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "\n",
    "\n",
    "    #Decision Tree)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_decision_tree]=cross_validation('Decision Tree',model['Decision Tree'],cross_val_X,cross_val_Y)\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_random_forest]=random_forest(train_X,train_Y,cross_val_X,cross_val_Y)\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    cross_val_X=preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    #Linear Regression\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_linear_regression]=cross_validation('Linear Regression',model['Linear Regression'],cross_val_X,cross_val_Y)\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_logistic_regression_L2]=cross_validation('L2',model['L2'],cross_val_X,cross_val_Y)\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L1\n",
    "    #The AUC (Cross Validation Data)\n",
    "    [auc,predict_logistic_regression_L1]=cross_validation('L1',model['L1'],cross_val_X,cross_val_Y)\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    predict_list=[predict_XGB,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1]\n",
    "    \n",
    "    #Stacking (XGBoost - Gradient Boosting)\n",
    "    stack_X=build_data_frame(predict_list)\n",
    "    [auc,predict_XGB]=cross_validation('Gradient Boosting',model_stack['Stack'],xgb.DMatrix(stack_X,label=cross_val_Y),cross_val_Y)\n",
    "    metric_stack.append(auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_linear_regression=np.mean(metric_linear_regression)\n",
    "avg_logistic_regression_L2=np.mean(metric_logistic_regression_L2)\n",
    "avg_logistic_regression_L1=np.mean(metric_logistic_regression_L1)\n",
    "avg_decision_tree=np.mean(metric_decision_tree)\n",
    "avg_random_forest=np.mean(metric_random_forest)\n",
    "avg_multi_layer_perceptron=np.mean(metric_multi_layer_perceptron)\n",
    "avg_XGB=np.mean(metric_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC (Linear Regression)\n",
      " 0.930177952129\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.932707310756\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.932701625385\n",
      " AUC (Decision Tree)\n",
      " 0.924629977069\n",
      " AUC (Random Forest)\n",
      " 0.912628473604\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.869175178931\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.946932426201\n",
      "[0.93983518739616301]\n"
     ]
    }
   ],
   "source": [
    "print (' AUC (Linear Regression)\\n',avg_linear_regression)\n",
    "print (' AUC (Logistic Regression - L2)\\n',avg_logistic_regression_L2)\n",
    "print (' AUC (Logistic Regression - L1)\\n',avg_logistic_regression_L1)\n",
    "print (' AUC (Decision Tree)\\n',avg_decision_tree)\n",
    "print (' AUC (Random Forest)\\n',avg_random_forest)\n",
    "print (' AUC (Multi Layer Perceptron)\\n',avg_multi_layer_perceptron)\n",
    "print (' AUC (Gradient Boosting - XGBoost)\\n',avg_XGB)\n",
    "print(metric_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
