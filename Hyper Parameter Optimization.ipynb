{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing, grid_search\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2, activity_l2\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "import category_encoders as ce\n",
    "np.random.seed(1338)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Data_import():\n",
    "    \n",
    "    global Data\n",
    "    #Reading the data, into a Data Frame.\n",
    "    Data = pd.read_csv('/home/prajwal/Desktop/bank-additional/bank-additional-full.csv',delimiter=';',header=0)\n",
    "\n",
    "    #Selcting the columns of string data type\n",
    "    names = Data.select_dtypes(include = ['object'])\n",
    "    \n",
    "    #Converting string categorical variables to integer categorical variables.\n",
    "    label_encode(names.columns.tolist())\n",
    "    \n",
    "    global columns\n",
    "    columns = names.drop(['y'],axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function that encodes the string values to numerical values.\n",
    "def label_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values.\n",
    "    encoder = ce.OrdinalEncoder(verbose=1, cols=column_names)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encoding the data, encoding the string values into numerical values, using one hot encoding method.\n",
    "def perform_one_hot_encoding():\n",
    "    \n",
    "    global Data\n",
    "    #Finding the one hot encoding for all columns excpet the target label column.\n",
    "    for column in columns:\n",
    "    \n",
    "        #Converting the data frame column to an array\n",
    "        column_list = np.array(Data[column].tolist())\n",
    "        #Converting the array to a nested list for the one hot encode transformation\n",
    "        column_list_of_list = np.reshape(column_list,(-1,1))\n",
    "    \n",
    "        #Storing the one hot encode data frame\n",
    "        data_frame_one_hot = one_hot_encode(column_list_of_list,column)\n",
    "        \n",
    "        #Dropping the original column and then replacing it with the one hot encoded data frame.\n",
    "        Data = Data.drop([column],axis=1)\n",
    "        Data = pd.concat([Data, data_frame_one_hot], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(X,column_name):\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    X = enc.fit_transform(X).toarray()\n",
    "    #Converting X to a dataframe.\n",
    "    X = pd.DataFrame(X)\n",
    "    #Assigning column names to the one hot encoded columns.\n",
    "    X = X.rename(columns=lambda x: column_name + str(x))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def one_hot_encode(column_names):\n",
    "    \n",
    "    #global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using one-hot method\n",
    "    #encoder = ce.OneHotEncoder(cols=column_names)\n",
    "    #Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binary_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using binary method.\n",
    "    encoder = ce.BinaryEncoder(cols=column_names,verbose=1)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashing_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using hashing method.\n",
    "    encoder = ce.HashingEncoder(verbose=1, n_components=128, cols=column_names)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Difference Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_difference_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using backward difference method.\n",
    "    encoder = ce.BackwardDifferenceEncoder(verbose=1, cols=column_names)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helmert Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def helmert_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using helmert method.\n",
    "    encoder = ce.HelmertEncoder(verbose=1, cols=column_names)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using sum method.\n",
    "    encoder = ce.SumEncoder(verbose=1, cols=column_names)\n",
    "    Data = encoder.fit_transform(Data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_encode(column_names):\n",
    "    \n",
    "    global Data\n",
    "    #Encoding the data, encoding the string values into numerical values, using polynomial method.\n",
    "    encoder = ce.PolynomialEncoder(verbose=1, cols=column_names)\n",
    "    Data = encoder.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_one_hot_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        perform_one_hot_encoding()\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_label_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_hashing_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        hashing_encode(columns)\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_backward_difference_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        backward_difference_encode(columns)\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_helmert_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        helmert_encode(columns)\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_binary_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        binary_encode(columns)\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_sum_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        sum_encode(columns)\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing training, cross validation and testing on different stratified splits of the data.\n",
    "def sample_generation_polynomial_encode(n):\n",
    "    \n",
    "    for i in range(n):\n",
    "        Data_import()\n",
    "        polynomial_encode(columns)\n",
    "        data_initialize()\n",
    "        data_split()\n",
    "        metric_initialize()\n",
    "        train_cross_val_base_models()\n",
    "        print_metric_cross_val(i)\n",
    "        train_second_level_models()\n",
    "        metric_initialize()\n",
    "        test_data()\n",
    "        print_metric_test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing datasets (Stratified Split)\n",
    "def data_split():\n",
    "    \n",
    "\n",
    "    global Data\n",
    "    global test\n",
    "    Data, test = train_test_split(Data, test_size = 0.1, stratify = Data['y'],random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function is used to convert the predictions of the base models into a DataFrame.\n",
    "def build_data_frame(data):\n",
    "    \n",
    "    data_frame = pd.DataFrame(data).T\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_initialize():\n",
    "    \n",
    "    #Initializing the test dataset.\n",
    "    test = pd.DataFrame()\n",
    "    \n",
    "    global stack_X\n",
    "    global stack_Y\n",
    "    \n",
    "    #Initializing two data frames that will be used as training data for the stacked model.\n",
    "    #The data frame will contain the predictions of the base models.\n",
    "    stack_X = pd.DataFrame() \n",
    "    #The data frame will contain the calss labels of the base models.\n",
    "    stack_Y = pd.DataFrame() \n",
    "    \n",
    "    global blend_X\n",
    "    global raw_features_X\n",
    "    \n",
    "    #Initializing two data frames that will be used as training data for the blending model.\n",
    "    #The data frames will contain the predictions and raw features  of the base models.\n",
    "    blend_X = pd.DataFrame() \n",
    "    #The data frames will contain the raw features  of the data, which will be concatenated with the predictions.\n",
    "    raw_features_X = pd.DataFrame() \n",
    "    \n",
    "    global test_blend_X\n",
    "    global test_raw_features_X\n",
    "    global test_stack_X \n",
    "    global test_stack_Y\n",
    "    \n",
    "    #Initializing the dataframes that will be used for testing the stacking and blending models.\n",
    "    test_blend_X = pd.DataFrame()\n",
    "    test_raw_features_X = pd.DataFrame()\n",
    "    test_stack_X = pd.DataFrame()\n",
    "    test_stack_Y = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the parameters for the XGBoost (Gradient Boosting) Algorithm.\n",
    "def param_set_XGBoost():\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster which will be optimized later using hyperopt.\n",
    "    param['booster'] = ['gbtree','gblinear']\n",
    "    param['objective'] = ['binary:logistic']\n",
    "    param[\"eval_metric\"] = [\"auc\"]\n",
    "    param['eta'] = [0.1,0.3,0.5,0.7,0.9,1]\n",
    "    param['gamma'] = [0,1,5,10,15]\n",
    "    param['max_depth'] = [3,6,9,12,15,20]\n",
    "    param['min_child_weight'] = [1,5,10]\n",
    "    param['max_delta_step'] = [0,1,5,10]\n",
    "    param['subsample'] = [0.5,1]\n",
    "    param['colsample_bytree'] = [0.5,1]\n",
    "    param['silent'] = [1]\n",
    "    param['seed'] = [0]\n",
    "    param['base_score'] = [0.5]\n",
    "    param['lambda_bias'] = [1,5,10]\n",
    "    param['lambda'] = [0,0.1,0.5,1,10]\n",
    "    \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning the values of the XGBoost parameters that need to be checked, for minimizing the objective (loss).\n",
    "#The values that give the most optimum results will be picked to train the model.\n",
    "def assign_space_gradient_boosting():\n",
    "    \n",
    "    parameter_gradient_boosting = param_set_XGBoost()\n",
    "    space_gradient_boosting ={\n",
    "        \n",
    "        'booster': hp.choice('booster', parameter_gradient_boosting['booster']),\n",
    "        \n",
    "        'eta': hp.choice('eta', parameter_gradient_boosting['eta']),\n",
    "        \n",
    "        'gamma': hp.choice('gamma', parameter_gradient_boosting['gamma']),\n",
    "        \n",
    "        'max_depth': hp.choice('max_depth', parameter_gradient_boosting['max_depth']),\n",
    "        \n",
    "        'min_child_weight': hp.choice('min_child_weight', parameter_gradient_boosting['min_child_weight']),\n",
    "        \n",
    "        'max_delta_step': hp.choice('max_delta_step', parameter_gradient_boosting['max_delta_step']),\n",
    "        \n",
    "        'subsample': hp.choice('subsample', parameter_gradient_boosting['subsample']),\n",
    "        \n",
    "        'colsample_bytree': hp.choice('colsample_bytree', parameter_gradient_boosting['colsample_bytree']),\n",
    "        \n",
    "        'silent': hp.choice('silent', parameter_gradient_boosting['silent']),\n",
    "        \n",
    "        'seed': hp.choice('seed', parameter_gradient_boosting['seed']),\n",
    "        \n",
    "        'base_score': hp.choice('base_score', parameter_gradient_boosting['base_score']),\n",
    "        \n",
    "        'lambda_bias': hp.choice('lambda_bias', parameter_gradient_boosting['lambda_bias']),\n",
    "        \n",
    "        'lambda': hp.choice('lambda', parameter_gradient_boosting['lambda'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return space_gradient_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function calculates the loss for different parameter values and is used to determine the most optimum \n",
    "#parameter values\n",
    "def objective_gradient_boosting(space_gradient_boosting):\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = space_gradient_boosting['booster']\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = space_gradient_boosting['eta']\n",
    "    param['gamma'] = space_gradient_boosting['gamma']\n",
    "    param['max_depth'] = space_gradient_boosting['max_depth']\n",
    "    param['min_child_weight'] = space_gradient_boosting['min_child_weight']\n",
    "    param['max_delta_step'] = space_gradient_boosting['max_delta_step']\n",
    "    param['subsample'] = space_gradient_boosting['subsample']\n",
    "    param['colsample_bytree'] = space_gradient_boosting['colsample_bytree']\n",
    "    param['silent'] = space_gradient_boosting['silent']\n",
    "    param['seed'] = space_gradient_boosting['seed']\n",
    "    param['base_score'] = space_gradient_boosting['base_score']\n",
    "    param['lambda_bias'] = space_gradient_boosting['lambda_bias']\n",
    "    param['lambda'] = space_gradient_boosting['lambda']\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    auc_list = list()\n",
    "    \n",
    "    #Declared train_X as a global variable, unable to pass it as a parameter\n",
    "    #Performing cross validation.\n",
    "    skf=StratifiedKFold(train_Y, n_folds=3,random_state=0)\n",
    "    for train_index, cross_val_index in skf:\n",
    "        \n",
    "        xgb_train_X, xgb_cross_val_X = train_X.iloc[train_index],train_X.iloc[cross_val_index]\n",
    "        xgb_train_Y, xgb_cross_val_Y = train_Y.iloc[train_index],train_Y.iloc[cross_val_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(xgb_train_X, label = xgb_train_Y)\n",
    "        model = xgb.train(param, dtrain)\n",
    "        \n",
    "        predict = model.predict(xgb.DMatrix(xgb_cross_val_X, label = xgb_cross_val_Y))\n",
    "        auc_list.append(roc_auc_score(xgb_cross_val_Y,predict))\n",
    "    \n",
    "    #Calculating the AUC and returning the loss, which will be minimised by selecting the optimum parameters.\n",
    "    auc = np.mean(auc_list)\n",
    "    return{'loss':1-auc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the loss values, this function picks the optimum parameter values. These values will be used \n",
    "#for training the model\n",
    "def gradient_boosting_parameters(train_X,train_Y,obj):\n",
    "    \n",
    "    space_gradient_boosting = assign_space_gradient_boosting()\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn = obj,\n",
    "    space = space_gradient_boosting,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 100,\n",
    "    trials = trials)\n",
    "    \n",
    "    parameter_gradient_boosting = param_set_XGBoost()\n",
    "    optimal_param={}\n",
    "    #Best is a dictionary that contains the indices of the optimal parameter values.\n",
    "    #The following for loop uses these indices to obtain the parameter values, these values are stored in a\n",
    "    #dictionary - optimal_param\n",
    "    for key in best:\n",
    "        optimal_param[key] = parameter_gradient_boosting[key][best[key]]\n",
    "        \n",
    "    #optimal_param['booster'] = 'gbtree'\n",
    "    optimal_param['objective'] = 'binary:logistic'\n",
    "    optimal_param[\"eval_metric\"] = \"auc\"\n",
    "    \n",
    "    #Training the model with the optimal parameter values\n",
    "    dtrain = xgb.DMatrix(train_X, label = train_Y)\n",
    "    model = xgb.train(optimal_param, dtrain)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trains the Gradient Boosting model.\n",
    "def train_gradient_boosting(train_X,train_Y):\n",
    "    \n",
    "    model = gradient_boosting_parameters(train_X,train_Y,objective_gradient_boosting)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_val_gradient_boosting(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    predict = gradient_boosting.predict(xgb.DMatrix(cross_val_X, label = cross_val_Y))\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Multi Layer Perceptron model.\n",
    "def train_multi_layer_perceptron(train_X,train_Y):\n",
    "    \n",
    "    model = multi_layer_perceptron_parameters(train_X,train_Y,objective_multi_layer_perceptron)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_multi_layer_perceptron(cross_val_X,cross_val_Y):\n",
    "\n",
    "    global multi_layer_perceptron\n",
    "    cross_val_X = StandardScaler().fit_transform(cross_val_X)\n",
    "    \n",
    "    predict = multi_layer_perceptron.predict(cross_val_X)\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the parameters for the XGBoost (Gradient Boosting) Algorithm.\n",
    "def param_set_multi_layer_perceptron():\n",
    "    \n",
    "    param={}\n",
    "    param['dim_layer'] = [32,64]\n",
    "    param['activation_layer_1'] = ['sigmoid','relu']\n",
    "    param['init_layer_1'] = ['normal','uniform']\n",
    "    param['activation_layer_2'] = ['sigmoid','relu']\n",
    "    param['optimizer'] = ['rmsprop']\n",
    "    param['dropout'] = [0.2,0.5]\n",
    "    \n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assigning the values of the multi layer perceptron parameters that need to be checked, \n",
    "#for minimizing the objective (loss). \n",
    "#The values that give the most optimum results will be picked to train the model.\n",
    "def assign_space_multi_layer_perceptron():\n",
    "    \n",
    "    parameter_multi_layer_perceptron = param_set_multi_layer_perceptron()\n",
    "    space_multi_layer_perceptron ={\n",
    "        \n",
    "        'dim_layer': hp.choice('dim_layer', parameter_multi_layer_perceptron['dim_layer']),\n",
    "        \n",
    "        'activation_layer_1': hp.choice('activation_layer_1', parameter_multi_layer_perceptron['activation_layer_1']),\n",
    "        \n",
    "        'init_layer_1': hp.choice('init_layer_1', parameter_multi_layer_perceptron['init_layer_1']),\n",
    "        \n",
    "        'activation_layer_2': hp.choice('activation_layer_2', parameter_multi_layer_perceptron['activation_layer_2']),\n",
    "        \n",
    "        'optimizer': hp.choice('optimizer', parameter_multi_layer_perceptron['optimizer']),\n",
    "        \n",
    "        'dropout': hp.choice('dropout', parameter_multi_layer_perceptron['dropout'])\n",
    "        \n",
    "        \n",
    "    }\n",
    "    \n",
    "    return space_multi_layer_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function calculates the loss for different parameter values and is used to determine the most optimum \n",
    "#parameter values\n",
    "def objective_multi_layer_perceptron(space_multi_layer_perceptron):\n",
    "    \n",
    "    #Setting Parameters for the MLP model.\n",
    "    dim_layer = space_multi_layer_perceptron['dim_layer']\n",
    "    activation_layer_1 = space_multi_layer_perceptron['activation_layer_1']\n",
    "    init_layer_1 = space_multi_layer_perceptron['init_layer_1']\n",
    "    activation_layer_2 = space_multi_layer_perceptron['activation_layer_2']\n",
    "    optimizer = space_multi_layer_perceptron['optimizer']\n",
    "    dropout = space_multi_layer_perceptron['dropout']\n",
    "\n",
    "    \n",
    "    acc_list = list()\n",
    "    \n",
    "    #Declared train_X as a global variable, unable to pass it as a parameter\n",
    "    #Performing cross validation.\n",
    "    skf=StratifiedKFold(train_Y, n_folds = 3,random_state=0)\n",
    "    for train_index, cross_val_index in skf:\n",
    "        \n",
    "        mlp_train_X, mlp_cross_val_X = train_X.iloc[train_index],train_X.iloc[cross_val_index]\n",
    "        mlp_train_Y, mlp_cross_val_Y = train_Y.iloc[train_index],train_Y.iloc[cross_val_index]\n",
    "        #mlp_train_X = mlp_train_X.as_matrix()\n",
    "        mlp_train_Y = mlp_train_Y.as_matrix()\n",
    "        mlp_train_X = StandardScaler().fit_transform(mlp_train_X)\n",
    "        mlp_cross_val_X = StandardScaler().fit_transform(mlp_cross_val_X)\n",
    "\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(output_dim = dim_layer, input_dim = train_X.shape[1], init = init_layer_1\n",
    "                        , activation = activation_layer_1))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(output_dim = 1,activation = activation_layer_2))\n",
    "        model.compile(optimizer = optimizer,loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "        model.fit(mlp_train_X, mlp_train_Y, nb_epoch = 2, batch_size = 128,verbose=1)\n",
    "\n",
    "        #predict = model.predict(mlp_cross_val_X)\n",
    "        #auc_list.append(roc_auc_score(mlp_cross_val_Y,predict))\n",
    "        \n",
    "        score = model.evaluate(mlp_cross_val_X, mlp_cross_val_Y, verbose=0)\n",
    "        acc_list.append(score[1])\n",
    "    \n",
    "    #Calculating the AUC and returning the loss, which will be minimised by selecting the optimum parameters.\n",
    "    acc = np.mean(acc_list)\n",
    "    return{'loss':1-acc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using the loss values, this function picks the optimum parameter values. These values will be used \n",
    "#for training the model\n",
    "def multi_layer_perceptron_parameters(train_X,train_Y,obj):\n",
    "    \n",
    "    space_multi_layer_perceptron = assign_space_multi_layer_perceptron()\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn = obj,\n",
    "    space = space_multi_layer_perceptron,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 5,\n",
    "    trials = trials)\n",
    "    \n",
    "    parameter_multi_layer_perceptron = param_set_multi_layer_perceptron()\n",
    "    optimal_param={}\n",
    "    \n",
    "    #Best is a dictionary that contains the indices of the optimal parameter values.\n",
    "    #The following for loop uses these indices to obtain the parameter values, these values are stored in a\n",
    "    #dictionary - optimal_param\n",
    "    for key in best:\n",
    "        optimal_param[key] = parameter_multi_layer_perceptron[key][best[key]]\n",
    "    \n",
    "    train_X = StandardScaler().fit_transform(train_X)\n",
    "    #Training the model with the optimal parameter values\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim = optimal_param['dim_layer'] , \n",
    "                    input_dim = train_X.shape[1], init = optimal_param['init_layer_1'], \n",
    "                    activation = optimal_param['activation_layer_1']))\n",
    "    model.add(Dropout(optimal_param['dropout']))\n",
    "    model.add(Dense(output_dim = 1,\n",
    "                    activation = optimal_param['activation_layer_2']))\n",
    "    model.compile(optimizer = optimal_param['optimizer'],loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
    "    model.fit(train_X, train_Y, nb_epoch = 15, batch_size = 128,verbose=1)\n",
    "    cross = StandardScaler().fit_transform(cross_val_X)\n",
    "    x = model.predict(cross)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Decision Tree model. Performing a grid search to select the optimal parameter values\n",
    "def train_decision_tree(train_X,train_Y):\n",
    "    \n",
    "    pipeline_model = Pipeline([('dtc', DecisionTreeClassifier())])\n",
    "    param = {'dtc__max_depth':[6,9,12,15,20],'dtc__criterion':['gini','entropy'],}       \n",
    "    model_gs = grid_search.GridSearchCV(pipeline_model, param,scoring='roc_auc')\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_decision_tree(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    global decision_tree\n",
    "    predict = decision_tree.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def decision_tree_parameters(parameters_decision_tree={}):\n",
    "    \n",
    "    #param = parameters_decision_tree\n",
    "    #return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trains the Random Forest model. Performing a grid search to select the optimal parameter values\n",
    "def train_random_forest(train_X,train_Y):\n",
    "    \n",
    "    pipeline_model = Pipeline([('rfc', RandomForestClassifier())]) \n",
    "    param = {'rfc__max_depth':[6,9,12,15,20],'rfc__n_estimators':[5,10,15,20]}\n",
    "    model_gs = grid_search.GridSearchCV(pipeline_model, param, scoring='roc_auc')\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_random_forest(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    global random_forest\n",
    "    predict = random_forest.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def random_forest_parameters(parameters_random_forest={}):\n",
    "    \n",
    "    #param = parameters_random_forest\n",
    "    #return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trains the Linear Regression model. Performing a grid search to select the optimal parameter values\n",
    "def train_linear_regression(train_X,train_Y):\n",
    "    \n",
    "    pipeline_model = Pipeline([('scl', StandardScaler()),('lr', linear_model.LinearRegression())]) \n",
    "    param = {'lr__normalize':[False]}\n",
    "    #train_X=StandardScaler().fit_transform(train_X)\n",
    "    model_gs = grid_search.GridSearchCV(pipeline_model, param, scoring='roc_auc')\n",
    "    model_gs.fit(train_X,train_Y) \n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_linear_regression(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    #cross_val_X = StandardScaler().fit_transform(cross_val_X)\n",
    "    predict = linear_regression.predict(cross_val_X)\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def linear_regression_parameters(parameters_linear_regression={}):\n",
    "    \n",
    "    #param = parameters_linear_regression\n",
    "    #return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losgistic Regression (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trains the Logistic Regression (L2) model. Performing a grid search to select the optimal parameter values\n",
    "def train_logistic_regression_L1(train_X,train_Y):\n",
    "\n",
    "    pipeline_model = Pipeline([('scl', StandardScaler()),('l1', linear_model.LogisticRegression())]) \n",
    "    param = {'l1__penalty':['l1'],'l1__C':[0.0001,0.001,0.01,0.1,1,10,100,100]}\n",
    "    train_X=StandardScaler().fit_transform(train_X)\n",
    "    model_gs = grid_search.GridSearchCV(pipeline_model, param, scoring='roc_auc')\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_logistic_regression_L1(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    cross_val_X = preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    predict = logistic_regression_L1.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def logistic_regression_L1_parameters(parameters_logistic_regression_L1={}):\n",
    "    \n",
    "    #param = parameters_logistic_regression_L1\n",
    "    #return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Logistic Regression (L2) model. Performing a grid search to select the optimal parameter values\n",
    "def train_logistic_regression_L2(train_X,train_Y):\n",
    "    \n",
    "    pipeline_model = Pipeline([('scl', StandardScaler()),('l2', linear_model.LogisticRegression())]) \n",
    "    param = {'l2__penalty':['l2'],'l2__C':[0.0001,0.001,0.01,0.1,1,10,100,100]}\n",
    "    train_X=StandardScaler().fit_transform(train_X)\n",
    "    model_gs = grid_search.GridSearchCV(pipeline_model, param, scoring='roc_auc')\n",
    "    model_gs.fit(train_X,train_Y)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_logistic_regression_L2(cross_val_X,cross_val_Y):\n",
    "    \n",
    "    cross_val_X = preprocessing.StandardScaler().fit_transform(cross_val_X)\n",
    "    predict = logistic_regression_L2.predict_proba(cross_val_X)[:,1]\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def logistic_regression_L2_parameters(parameters_logistic_regression_L2={}):\n",
    "    \n",
    "    #param = parameters_logistic_regression_L2\n",
    "    #return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perfroms weighted average of the predictions of the base models.\n",
    "def weighted_average(data_frame_predictions, cross_val_Y):\n",
    "    \n",
    "    weighted_avg_predictions=np.average(data_frame_predictions,axis=1,weights=weight)\n",
    "    auc = roc_auc_score(cross_val_Y,weighted_avg_predictions)\n",
    "    return [auc,weighted_avg_predictions]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the objective. Appropriate weights need to be calculated to minimize the loss.\n",
    "def objective_weighted_average(space):\n",
    "    \n",
    "    global stack_X\n",
    "    weighted_avg_predictions=np.average(stack_X,axis=1,weights=[space['w1'],space['w2'],space['w3']\n",
    "                                                                              ,space['w4'],space['w5'],\n",
    "                                                                              space['w6'],space['w7']])\n",
    "    \n",
    "    \n",
    "    global stack_Y\n",
    "    auc = roc_auc_score(stack_Y,weighted_avg_predictions)\n",
    "    return{'loss':1-auc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assigning the weights that need to be checked, for minimizing the objective (Loss)\n",
    "def assign_space_weighted_average():\n",
    "    \n",
    "    space ={\n",
    "        'w1': hp.choice(\"x_w1\", range(10)),\n",
    "        'w2': hp.choice('x_w2', range(10)),\n",
    "        'w3': hp.choice('x_w3', range(10)),\n",
    "        'w4': hp.choice('x_w4', range(10)),\n",
    "        'w5': hp.choice('x_w5', range(10)),\n",
    "        'w6': hp.choice('x_w6', range(10)),\n",
    "        'w7': hp.choice('x_w7', range(10))\n",
    "    }\n",
    "    \n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function that finds the best possible combination of weights for performing the weighted predictions.\n",
    "def get_weights():\n",
    "    \n",
    "    space = assign_space_weighted_average()\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn = objective_weighted_average,\n",
    "    space = space,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 100,\n",
    "    trials = trials)\n",
    "    best_weights = list()\n",
    "    \n",
    "    #Arranging the weights in order of the respective models, and then returning the list of weights.\n",
    "    for key in sorted(best):\n",
    "        best_weights.append(best[key])\n",
    "    \n",
    "    return best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the Stacking model (Gradient Boosting - XGBoost)\n",
    "def train_stack_model(train_X,train_Y):\n",
    "    \n",
    "    model = gradient_boosting_parameters(train_X,train_Y,objective_stack)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_stack(cross_val_X,cross_val_Y):\n",
    "\n",
    "    predict = stack.predict(xgb.DMatrix(cross_val_X,label = cross_val_Y))\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function calculates the loss for different parameter values and is used to determine the most optimum \n",
    "#parameter values\n",
    "def objective_stack(space_gradient_boosting):\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = space_gradient_boosting['booster']\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = space_gradient_boosting['eta']\n",
    "    param['gamma'] = space_gradient_boosting['gamma']\n",
    "    param['max_depth'] = space_gradient_boosting['max_depth']\n",
    "    param['min_child_weight'] = space_gradient_boosting['min_child_weight']\n",
    "    param['max_delta_step'] = space_gradient_boosting['max_delta_step']\n",
    "    param['subsample'] = space_gradient_boosting['subsample']\n",
    "    param['colsample_bytree'] = space_gradient_boosting['colsample_bytree']\n",
    "    param['silent'] = space_gradient_boosting['silent']\n",
    "    param['seed'] = space_gradient_boosting['seed']\n",
    "    param['base_score'] = space_gradient_boosting['base_score']\n",
    "    param['lambda_bias'] = space_gradient_boosting['lambda_bias']\n",
    "    param['lambda'] = space_gradient_boosting['lambda']\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    auc_list = list()\n",
    "    \n",
    "    #Declared train_X as a global variable, unable to pass it as a parameter\n",
    "    #Performing cross validation.\n",
    "    skf=StratifiedKFold(stack_Y, n_folds=3,random_state=0)\n",
    "    for train_index, cross_val_index in skf:\n",
    "        \n",
    "        xgb_train_X, xgb_cross_val_X = stack_X.iloc[train_index],stack_X.iloc[cross_val_index]\n",
    "        xgb_train_Y, xgb_cross_val_Y = stack_Y.iloc[train_index],stack_Y.iloc[cross_val_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(xgb_train_X, label = xgb_train_Y)\n",
    "        model = xgb.train(param, dtrain)\n",
    "        \n",
    "        predict = model.predict(xgb.DMatrix(xgb_cross_val_X, label = xgb_cross_val_Y))\n",
    "        auc_list.append(roc_auc_score(xgb_cross_val_Y,predict))\n",
    "    \n",
    "    #Calculating the AUC and returning the loss, which will be minimised by selecting the optimum parameters.\n",
    "    auc = np.mean(auc_list)\n",
    "    return{'loss':1-auc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains the blending model (Gradient Boosting - XGBoost)\n",
    "def train_blend_model(train_X,train_Y): \n",
    "    \n",
    "    model = gradient_boosting_parameters(train_X,train_Y,objective_blend)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_blend(cross_val_X,cross_val_Y):\n",
    "\n",
    "    predict = blend.predict(xgb.DMatrix(cross_val_X,label = cross_val_Y))\n",
    "    auc = roc_auc_score(cross_val_Y,predict)\n",
    "    return [auc,predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function calculates the loss for different parameter values and is used to determine the most optimum \n",
    "#parameter values\n",
    "def objective_blend(space_gradient_boosting):\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    param = {}\n",
    "    #Setting Parameters for the Booster\n",
    "    param['booster'] = space_gradient_boosting['booster']\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param[\"eval_metric\"] = \"auc\"\n",
    "    param['eta'] = space_gradient_boosting['eta']\n",
    "    param['gamma'] = space_gradient_boosting['gamma']\n",
    "    param['max_depth'] = space_gradient_boosting['max_depth']\n",
    "    param['min_child_weight'] = space_gradient_boosting['min_child_weight']\n",
    "    param['max_delta_step'] = space_gradient_boosting['max_delta_step']\n",
    "    param['subsample'] = space_gradient_boosting['subsample']\n",
    "    param['colsample_bytree'] = space_gradient_boosting['colsample_bytree']\n",
    "    param['silent'] = space_gradient_boosting['silent']\n",
    "    param['seed'] = space_gradient_boosting['seed']\n",
    "    param['base_score'] = space_gradient_boosting['base_score']\n",
    "    param['lambda_bias'] = space_gradient_boosting['lambda_bias']\n",
    "    param['lambda'] = space_gradient_boosting['lambda']\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    auc_list = list()\n",
    "    \n",
    "    #Declared train_X as a global variable, unable to pass it as a parameter\n",
    "    #Performing cross validation.\n",
    "    skf=StratifiedKFold(stack_Y, n_folds=3,random_state=0)\n",
    "    for train_index, cross_val_index in skf:\n",
    "        \n",
    "        xgb_train_X, xgb_cross_val_X = blend_X.iloc[train_index],blend_X.iloc[cross_val_index]\n",
    "        xgb_train_Y, xgb_cross_val_Y = stack_Y.iloc[train_index],stack_Y.iloc[cross_val_index]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(xgb_train_X, label = xgb_train_Y)\n",
    "        model = xgb.train(param, dtrain)\n",
    "        \n",
    "        predict = model.predict(xgb.DMatrix(xgb_cross_val_X, label = xgb_cross_val_Y))\n",
    "        auc_list.append(roc_auc_score(xgb_cross_val_Y,predict))\n",
    "    \n",
    "    #Calculating the AUC and returning the loss, which will be minimised by selecting the optimum parameters.\n",
    "    auc = np.mean(auc_list)\n",
    "    return{'loss':1-auc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_initialize():\n",
    "    \n",
    "    global metric_linear_regression\n",
    "    global metric_logistic_regression_L2\n",
    "    global metric_logistic_regression_L1\n",
    "    global metric_decision_tree\n",
    "    global metric_random_forest\n",
    "    global metric_gradient_boosting\n",
    "    global metric_multi_layer_perceptron\n",
    "    global metric_stacking\n",
    "    global metric_blending\n",
    "    global metric_weighted_average\n",
    "    \n",
    "    #Initialzing the variables that will be used to calculate the area under the curve on the given data.\n",
    "    metric_linear_regression = list()\n",
    "    metric_logistic_regression_L2 = list()\n",
    "    metric_logistic_regression_L1 = list()\n",
    "    metric_decision_tree = list()\n",
    "    metric_random_forest = list()\n",
    "    metric_multi_layer_perceptron = list()\n",
    "    metric_gradient_boosting = list()\n",
    "    metric_weighted_average = list()\n",
    "    metric_stacking = list()\n",
    "    metric_blending = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The list of base model functions (Training).\n",
    "train_base_model_list = [train_gradient_boosting,train_multi_layer_perceptron,train_decision_tree,train_random_forest,\n",
    "                 train_linear_regression,train_logistic_regression_L1,train_logistic_regression_L2]\n",
    "\n",
    "#The list of base model functions (Cross Validation).\n",
    "cross_val_base_model_list = [cross_val_gradient_boosting,cross_val_multi_layer_perceptron,cross_val_decision_tree\n",
    "                           ,cross_val_random_forest,cross_val_linear_regression,cross_val_logistic_regression_L1\n",
    "                           ,cross_val_logistic_regression_L2]\n",
    "\n",
    "#The list of second level model functions.\n",
    "cross_val_second_level_model = [cross_val_stack,cross_val_blend,weighted_average]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_cross_val_base_models():\n",
    "    \n",
    "    #Cross Validation using Stratified K Fold\n",
    "    train, cross_val = train_test_split(Data, test_size = 0.5, stratify = Data['y'],random_state=0)\n",
    "    \n",
    "    #Training the base models, and calculating AUC on the cross validation data.\n",
    "    #Selecting the data (Traing Data & Cross Validation Data)\n",
    "    global train_X\n",
    "    global train_Y\n",
    "    train_Y = train['y']\n",
    "    train_X = train.drop(['y'],axis=1)\n",
    "    \n",
    "    global cross_val_X\n",
    "    global cross_val_Y\n",
    "    cross_val_Y = cross_val['y']\n",
    "    cross_val_X = cross_val.drop(['y'],axis=1)\n",
    "    \n",
    "    global gradient_boosting\n",
    "    global multi_layer_perceptron\n",
    "    global decision_tree\n",
    "    global random_forest\n",
    "    global linear_regression\n",
    "    global logistic_regression_L1\n",
    "    global logistic_regression_L2\n",
    "\n",
    "    #Training the base models parallely, the resulting models are stored which will be used for cross validation.\n",
    "    [gradient_boosting,multi_layer_perceptron,decision_tree,random_forest,linear_regression,logistic_regression_L1\n",
    "     ,logistic_regression_L2] = (Parallel(n_jobs = -1)(delayed(function)(train_X, train_Y)\\\n",
    "                                                   for function in train_base_model_list))\n",
    "    \n",
    "    #Computing the AUC and Predictions of all the base models on the cross validation data parallely.\n",
    "    auc_predict_cross_val = (Parallel(n_jobs = -1)(delayed(function)(cross_val_X,cross_val_Y)\\\n",
    "                                               for function in cross_val_base_model_list))\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Cross Validation Data)\n",
    "    auc,predict_gradient_boosting = auc_predict_cross_val[0][0],auc_predict_cross_val[0][1]\n",
    "    metric_gradient_boosting.append(auc)\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Cross Validation Data)\n",
    "    predict_mlp=list()\n",
    "    auc,predict_multi_layer_perceptron = auc_predict_cross_val[1][0],auc_predict_cross_val[1][1]\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "    \n",
    "    #predict_multi_layer_perceptron returns a list of lists containing the predictions,\n",
    "    #this cannot be converted to a dataframe.\n",
    "    #This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "    for i in predict_multi_layer_perceptron:\n",
    "        predict_mlp.append(float(i))\n",
    "    \n",
    "    #Decision Tree)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_decision_tree = auc_predict_cross_val[2][0],auc_predict_cross_val[2][1]\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_random_forest = auc_predict_cross_val[3][0],auc_predict_cross_val[3][1]\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    #Linear Regression\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_linear_regression = auc_predict_cross_val[4][0],auc_predict_cross_val[4][1]\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_logistic_regression_L1 = auc_predict_cross_val[5][0],auc_predict_cross_val[5][1]\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L2\n",
    "    #The AUC (Cross Validation Data)\n",
    "    auc,predict_logistic_regression_L2 = auc_predict_cross_val[6][0],auc_predict_cross_val[6][1]\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Building a list that contains all the predictions of the base models.\n",
    "    predict_list = [predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    \n",
    "    #Converting the above list of predictions into a dataframe, which will be used to train the stacking model.\n",
    "    global stack_X\n",
    "    stack_X = stack_X.append(build_data_frame(predict_list))\n",
    "    \n",
    "    #Building a list that contains all the raw features, used as cross validation data for the base models.\n",
    "    global raw_features_X\n",
    "    raw_features_X = raw_features_X.append(cross_val_X,ignore_index=True)\n",
    "    \n",
    "    #Storing the cross validation dataset labels in the variable stack_Y, \n",
    "    #which will be used later to train the stacking and blending models.\n",
    "    global stack_Y\n",
    "    stack_Y = cross_val_Y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_metric_cross_val(n):\n",
    "    \n",
    "    #Calculating the average AUC across all the AUC computed on the cross validation folds.\n",
    "    avg_linear_regression = np.mean(metric_linear_regression)\n",
    "    avg_logistic_regression_L2 = np.mean(metric_logistic_regression_L2)\n",
    "    avg_logistic_regression_L1 = np.mean(metric_logistic_regression_L1)\n",
    "    avg_decision_tree = np.mean(metric_decision_tree)\n",
    "    avg_random_forest = np.mean(metric_random_forest)\n",
    "    avg_multi_layer_perceptron = np.mean(metric_multi_layer_perceptron)\n",
    "    avg_gradient_boosting = np.mean(metric_gradient_boosting)\n",
    "    \n",
    "    #Printing the AUC for the base models.\n",
    "    print('\\nStart Cross Validation Sample',n,'\\n')\n",
    "    print (' AUC (Linear Regression)\\n',avg_linear_regression)\n",
    "    print (' AUC (Logistic Regression - L2)\\n',avg_logistic_regression_L2)\n",
    "    print (' AUC (Logistic Regression - L1)\\n',avg_logistic_regression_L1)\n",
    "    print (' AUC (Decision Tree)\\n',avg_decision_tree)\n",
    "    print (' AUC (Random Forest)\\n',avg_random_forest)\n",
    "    print (' AUC (Multi Layer Perceptron)\\n',avg_multi_layer_perceptron)\n",
    "    print (' AUC (Gradient Boosting - XGBoost)\\n',avg_gradient_boosting)\n",
    "    print('\\nEnd Cross Validation Sample',n,'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Running the second level models parallely\n",
    "def train_second_level_models():\n",
    "    \n",
    "    #Performing a weighted average of all the base models and calculating the resulting AUC.\n",
    "    global weight\n",
    "    weight = get_weights()\n",
    "    \n",
    "    #Converting the above list of predictions and raw features (Concatenate) into a dataframe, \n",
    "    #which will be used to train the blending model.\n",
    "    global blend_X\n",
    "    blend_X = pd.concat([raw_features_X, stack_X], axis = 1,ignore_index = True)\n",
    "    \n",
    "    #Training the Stacking and Blending models parallely using the \n",
    "    #predictions of base models on the cross validation data.\n",
    "    global stack\n",
    "    global blend\n",
    "    function_param = [(train_stack_model,stack_X,stack_Y),(train_blend_model,blend_X,stack_Y),\n",
    "                      (weighted_average,stack_X,stack_Y)]\n",
    "    [stack,blend,[auc,predict_weighted_average]] = Parallel(n_jobs = -1)(delayed(model_function)(train_X,train_Y)\\\n",
    "                                        for model_function,train_X,train_Y in function_param)\n",
    "    \n",
    "    #Calculating and printing the AUC for the weighted average models.\n",
    "    metric_weighted_average.append(auc)\n",
    "    print (' AUC (Weighted Average)\\n',metric_weighted_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_metric_test(n):\n",
    "    \n",
    "    print('\\nStart Test Sample',n,'\\n')\n",
    "    #Printing the AUC for all the models. (Test Data)\n",
    "    print (' AUC (Linear Regression)\\n',metric_linear_regression)\n",
    "    print (' AUC (Logistic Regression - L2)\\n',metric_logistic_regression_L2)\n",
    "    print (' AUC (Logistic Regression - L1)\\n',metric_logistic_regression_L1)\n",
    "    print (' AUC (Decision Tree)\\n',metric_decision_tree)\n",
    "    print (' AUC (Random Forest)\\n',metric_random_forest)\n",
    "    print (' AUC (Multi Layer Perceptron)\\n',metric_multi_layer_perceptron)\n",
    "    print (' AUC (Weighted Average)\\n',metric_weighted_average)\n",
    "    print (' AUC (Gradient Boosting - XGBoost)\\n',metric_gradient_boosting)\n",
    "    print (' AUC (Stacking)\\n',metric_stacking)\n",
    "    print (' AUC (Blending)\\n',metric_blending)\n",
    "    print('\\nEnd Test Sample',n,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Base and Second Level Models on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    \n",
    "    #Training the base models, and calculating AUC on the test data.\n",
    "    #Selecting the data (Test Data)\n",
    "    test_Y = test['y']\n",
    "    test_X = test.drop(['y'],axis=1)\n",
    "    \n",
    "    #Computing the AUC and Predictions of all the base models on the test data parallely.\n",
    "    auc_predict_test = (Parallel(n_jobs = -1)(delayed(function)(test_X,test_Y)\\\n",
    "                                          for function in cross_val_base_model_list))\n",
    "    \n",
    "    #Gradient Boosting (XGBoost)\n",
    "    #The AUC error (Test Data)\n",
    "    auc,predict_gradient_boosting = auc_predict_test[0][0],auc_predict_test[0][1]\n",
    "    metric_gradient_boosting.append(auc)\n",
    "    \n",
    "    #Multi Layer Perceptron\n",
    "    #The AUC (Test Data)\n",
    "    predict_mlp=list()\n",
    "    auc,predict_multi_layer_perceptron = auc_predict_test[1][0],auc_predict_test[1][1]\n",
    "    metric_multi_layer_perceptron.append(auc)\n",
    "    \n",
    "    #predict_multi_layer_perceptron returns a list of lists containing the predictions, \n",
    "    #this cannot be converted to a dataframe.\n",
    "    #This inner lists are converted to floats and then used to convert it to a dataframe.\n",
    "    for i in predict_multi_layer_perceptron:\n",
    "        predict_mlp.append(float(i))\n",
    "    \n",
    "    #Decision Tree)\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_decision_tree = auc_predict_test[2][0],auc_predict_test[2][1]\n",
    "    metric_decision_tree.append(auc)\n",
    "    \n",
    "    #Random Forest (Deafult=10 Trees)\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_random_forest = auc_predict_test[3][0],auc_predict_test[3][1]\n",
    "    metric_random_forest.append(auc)\n",
    "    \n",
    "    #Linear Regression\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_linear_regression = auc_predict_test[4][0],auc_predict_test[4][1]\n",
    "    metric_linear_regression.append(auc)\n",
    "    \n",
    "    #Logistic Regression (Default=l2)\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_logistic_regression_L1 = auc_predict_test[5][0],auc_predict_test[5][1]\n",
    "    metric_logistic_regression_L1.append(auc)\n",
    "    \n",
    "    #Logistic Regression-L2\n",
    "    #The AUC (Test Data)\n",
    "    auc,predict_logistic_regression_L2 = auc_predict_test[6][0],auc_predict_test[6][1]\n",
    "    metric_logistic_regression_L2.append(auc)\n",
    "    \n",
    "    #Building a list that contains all the predictions of the base models.\n",
    "    predict_list = [predict_gradient_boosting,predict_decision_tree,predict_random_forest, \n",
    "                               predict_linear_regression,predict_logistic_regression_L2,\n",
    "                               predict_logistic_regression_L1,predict_mlp]\n",
    "    global test_stack_X\n",
    "    global test_raw_features_X\n",
    "    global test_blend_X\n",
    "    \n",
    "    test_stack_X = build_data_frame(predict_list)\n",
    "    #Converting the list of predictions into a dataframe.\n",
    "    test_raw_features_X = test_raw_features_X.append(test_X,ignore_index = True)\n",
    "    #Converting the above list of predictions and raw features (Concatenate) into a dataframe\n",
    "    test_blend_X = pd.concat([test_raw_features_X, test_stack_X], axis = 1,ignore_index = True)\n",
    "\n",
    "        \n",
    "    #Computing the AUC and Predictions of the Stacking and Blending models on the test data parallely.\n",
    "    auc_predict_test_second_level = Parallel(n_jobs = -1)(delayed(function)(test_X, test_Y) \\\n",
    "                                                      for function,test_X in \\\n",
    "                                                      ((cross_val_second_level_model[0],test_stack_X),\\\n",
    "                                                       (cross_val_second_level_model[1],test_blend_X),\\\n",
    "                                                       (cross_val_second_level_model[2],test_stack_X)))\n",
    "\n",
    "    #Stacking (XGBoost - Gradient Boosting)\n",
    "    auc,predict_stack = auc_predict_test_second_level[0][0],auc_predict_test_second_level[0][1]\n",
    "    metric_stacking.append(auc)    \n",
    "\n",
    "    #Blending (XGBoost - Gradient Boosting)\n",
    "    auc,predict_blend = auc_predict_test_second_level[1][0],auc_predict_test_second_level[1][1]\n",
    "    metric_blending.append(auc)\n",
    "    \n",
    "    #Performing a weighted average of all the base models and calculating the resulting AUC.\n",
    "    auc,predict_weighted_average = auc_predict_test_second_level[2][0],auc_predict_test_second_level[2][1]\n",
    "    metric_weighted_average.append(auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE HOT ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3973 - acc: 0.8508     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2471 - acc: 0.9022     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3599 - acc: 0.8692     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2383 - acc: 0.9052     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3244 - acc: 0.8905     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2327 - acc: 0.9032     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3148 - acc: 0.8971     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2137 - acc: 0.9060     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3658 - acc: 0.8853     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2215 - acc: 0.9072     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3671 - acc: 0.8767     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2234 - acc: 0.9077     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4621 - acc: 0.8023     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2940 - acc: 0.8937     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.5450 - acc: 0.7276     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3013 - acc: 0.8944     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.5714 - acc: 0.6964     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3113 - acc: 0.8921     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.1496 - acc: 0.6611     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.8330 - acc: 0.6590     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.6292 - acc: 0.6431     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.8086 - acc: 0.6730     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.1499 - acc: 0.6357     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7311 - acc: 0.5990     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4348 - acc: 0.8603     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2711 - acc: 0.9014     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3928 - acc: 0.8818     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2524 - acc: 0.9058     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3836 - acc: 0.8780     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2523 - acc: 0.9039     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3315 - acc: 0.8829     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2109 - acc: 0.9080     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1989 - acc: 0.9118     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1915 - acc: 0.9133     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1870 - acc: 0.9138     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1818 - acc: 0.9157     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1780 - acc: 0.9181     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1756 - acc: 0.9187     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1749 - acc: 0.9203     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1729 - acc: 0.9207     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1730 - acc: 0.9203     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1709 - acc: 0.9226     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1698 - acc: 0.9222     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1700 - acc: 0.9223     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1673 - acc: 0.9247     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.935120127534\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.935122821081\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.935132343133\n",
      " AUC (Decision Tree)\n",
      " 0.926851739257\n",
      " AUC (Random Forest)\n",
      " 0.939182462208\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.939199875625\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.946313227301\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94650173190699394]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92976555497900848]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.9291069154205388]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92903261946318227]\n",
      " AUC (Decision Tree)\n",
      " [0.93243165951224116]\n",
      " AUC (Random Forest)\n",
      " [0.93853984857776307]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.938669866503137]\n",
      " AUC (Weighted Average)\n",
      " [0.94714668144723813]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94911817774423324]\n",
      " AUC (Stacking)\n",
      " [0.94617523232227918]\n",
      " AUC (Blending)\n",
      " [0.94669707297514039]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 5min 29s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ONE HOT ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_one_hot_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4150 - acc: 0.8632     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2798 - acc: 0.8950     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3469 - acc: 0.8885     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2641 - acc: 0.8977     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3862 - acc: 0.8786     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2745 - acc: 0.8956     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 11.4137 - acc: 0.1767     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 3.7801 - acc: 0.4571     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.7639 - acc: 0.2612     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.6540 - acc: 0.1916     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.9326 - acc: 0.4455     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.4351 - acc: 0.3605     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.6284 - acc: 0.7480     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4138 - acc: 0.7670     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.6914 - acc: 0.6976     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4226 - acc: 0.7880     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 6.5096 - acc: 0.1509     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.7939 - acc: 0.4016     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4727 - acc: 0.8406     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2913 - acc: 0.8976     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4668 - acc: 0.8606     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2857 - acc: 0.9000     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4373 - acc: 0.8566     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2769 - acc: 0.9008     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3753 - acc: 0.8520     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2779 - acc: 0.8949     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3482 - acc: 0.8794     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2777 - acc: 0.8960     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3532 - acc: 0.8731     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2793 - acc: 0.8946     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.4070 - acc: 0.8650     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2555 - acc: 0.9037     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2311 - acc: 0.9055     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2172 - acc: 0.9085     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2106 - acc: 0.9078     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2053 - acc: 0.9111     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.2037 - acc: 0.9101     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.2026 - acc: 0.9102     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.2022 - acc: 0.9098     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2012 - acc: 0.9112     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1992 - acc: 0.9111     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.1964 - acc: 0.9112     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.1977 - acc: 0.9100     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1984 - acc: 0.9117     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1957 - acc: 0.9119     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.925454793023\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.927019903943\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.927389094589\n",
      " AUC (Decision Tree)\n",
      " 0.930645024993\n",
      " AUC (Random Forest)\n",
      " 0.940163248163\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.934480475658\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.945334727222\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94560630955068259]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92513856785697435]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.92607611208075846]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92678487192792103]\n",
      " AUC (Decision Tree)\n",
      " [0.93623195669607051]\n",
      " AUC (Random Forest)\n",
      " [0.94044589367422993]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.93650142695410155]\n",
      " AUC (Weighted Average)\n",
      " [0.94823635548846652]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94918303929430636]\n",
      " AUC (Stacking)\n",
      " [0.94686188027737161]\n",
      " AUC (Blending)\n",
      " [0.94901528373979904]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 4min 33s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LABEL ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_label_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2991 - acc: 0.8987     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2700 - acc: 0.9032     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3154 - acc: 0.9021     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2698 - acc: 0.9062     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3517 - acc: 0.8975     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2705 - acc: 0.9014     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3907 - acc: 0.8990     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3113 - acc: 0.9047     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3298 - acc: 0.9025     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2676 - acc: 0.9029     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3145 - acc: 0.8990     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2650 - acc: 0.9043     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3533 - acc: 0.8829     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2872 - acc: 0.8926     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4689 - acc: 0.7977     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2966 - acc: 0.8944     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3716 - acc: 0.8705     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2941 - acc: 0.8922     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.7695 - acc: 0.8704     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.5590 - acc: 0.8923     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.9455 - acc: 0.8788     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.6495 - acc: 0.8912     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.0565 - acc: 0.8763     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.6856 - acc: 0.8845     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.0786 - acc: 0.8650     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7664 - acc: 0.8721     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.8159 - acc: 0.8873     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.8158 - acc: 0.8873     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.0687 - acc: 0.8676     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.7659 - acc: 0.8699     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3502 - acc: 0.9025     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2681 - acc: 0.9075     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2598 - acc: 0.9068     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2456 - acc: 0.9087     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2545 - acc: 0.9089     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2354 - acc: 0.9085     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2401 - acc: 0.9075     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2429 - acc: 0.9108     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2379 - acc: 0.9106     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2430 - acc: 0.9112     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.2404 - acc: 0.9105     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2407 - acc: 0.9125     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2412 - acc: 0.9105     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2404 - acc: 0.9102     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2339 - acc: 0.9126     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.928682943494\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.930751441936\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.930747525373\n",
      " AUC (Decision Tree)\n",
      " 0.92835376294\n",
      " AUC (Random Forest)\n",
      " 0.938516704767\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.916092082148\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.9468198034\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94704285820446821]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92899016463040707]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.9315663474692204]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.93156634746922029]\n",
      " AUC (Decision Tree)\n",
      " [0.92945746261616113]\n",
      " AUC (Random Forest)\n",
      " [0.93297531723194482]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.92001450540119822]\n",
      " AUC (Weighted Average)\n",
      " [0.94457698476343221]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.9451507146563517]\n",
      " AUC (Stacking)\n",
      " [0.94458317609321207]\n",
      " AUC (Blending)\n",
      " [0.94554430633520437]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 4min 53s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('BINARY ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_binary_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASHING ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4398 - acc: 0.8716     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2711 - acc: 0.9009     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4502 - acc: 0.8606     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2814 - acc: 0.9021     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4867 - acc: 0.8444     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3024 - acc: 0.9000     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4815 - acc: 0.8974     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3130 - acc: 0.8989     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3801 - acc: 0.8973     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2988 - acc: 0.9034     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.4037 - acc: 0.8912     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2978 - acc: 0.9000     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3143 - acc: 0.8895     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2586 - acc: 0.9002     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3353 - acc: 0.8874     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2570 - acc: 0.9009     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3074 - acc: 0.8903     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2539 - acc: 0.8995     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4333 - acc: 0.8996     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2797 - acc: 0.9052     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2572 - acc: 0.8992     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2387 - acc: 0.9060     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2655 - acc: 0.8991     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2378 - acc: 0.9031     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4705 - acc: 0.8455     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2855 - acc: 0.8998     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3868 - acc: 0.8918     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2508 - acc: 0.9014     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.5043 - acc: 0.8202     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3114 - acc: 0.9013     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3666 - acc: 0.8969     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2798 - acc: 0.9039     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2734 - acc: 0.9021     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2654 - acc: 0.9043     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2598 - acc: 0.9037     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2705 - acc: 0.9061     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2707 - acc: 0.9058     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.2706 - acc: 0.9057     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2703 - acc: 0.9069     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2669 - acc: 0.9086     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2620 - acc: 0.9075     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2607 - acc: 0.9079     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2632 - acc: 0.9087     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 0s - loss: 0.2719 - acc: 0.9081     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2626 - acc: 0.9085     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.930007658991\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.932025009161\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.932023640548\n",
      " AUC (Decision Tree)\n",
      " 0.928845837564\n",
      " AUC (Random Forest)\n",
      " 0.931911516197\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.91616306075\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.945760307636\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94590328401900259]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92713925185150237]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.92840287277701772]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92840287277701772]\n",
      " AUC (Decision Tree)\n",
      " [0.92643108165479504]\n",
      " AUC (Random Forest)\n",
      " [0.92700392707203172]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.9166912354356338]\n",
      " AUC (Weighted Average)\n",
      " [0.94292950139157505]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94258691447709808]\n",
      " AUC (Stacking)\n",
      " [0.94196601254776158]\n",
      " AUC (Blending)\n",
      " [0.94369339355629978]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 5min 7s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('HASHING ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_hashing_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKWARD DIFFERENCE ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4571 - acc: 0.8022     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2965 - acc: 0.8949     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3620 - acc: 0.8700     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2852 - acc: 0.8955     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4468 - acc: 0.8059     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2912 - acc: 0.8949     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7716 - acc: 0.8872     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3997 - acc: 0.8956     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.6167 - acc: 0.8885     \n",
      "Epoch 2/2\n",
      "12160/12356 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8958\n",
      "12356/12356 [==============================] - 0s - loss: 0.4028 - acc: 0.8963     Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7968 - acc: 0.8825     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.5494 - acc: 0.8933     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.8148 - acc: 0.8873     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.8148 - acc: 0.8873     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.5850 - acc: 0.8569     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.2929 - acc: 0.8712     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 2.0249 - acc: 0.7532     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3667 - acc: 0.8563     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3292 - acc: 0.8930     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2226 - acc: 0.9062     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3005 - acc: 0.8990     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2134 - acc: 0.9090     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.3411 - acc: 0.8880     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2243 - acc: 0.9060     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.6162 - acc: 0.8955     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3239 - acc: 0.9043     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4507 - acc: 0.8984     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2980 - acc: 0.9054     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4330 - acc: 0.8962     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3365 - acc: 0.9033     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3239 - acc: 0.8859     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2159 - acc: 0.9082     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2055 - acc: 0.9111     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1981 - acc: 0.9097     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1915 - acc: 0.9123     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1878 - acc: 0.9119     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1846 - acc: 0.9144     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1828 - acc: 0.9139     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1808 - acc: 0.9155     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1794 - acc: 0.9151     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1789 - acc: 0.9160     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1779 - acc: 0.9176     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1762 - acc: 0.9177     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1754 - acc: 0.9184     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1736 - acc: 0.9182     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.932698657967\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.934598409334\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.934678211121\n",
      " AUC (Decision Tree)\n",
      " 0.928808244385\n",
      " AUC (Random Forest)\n",
      " 0.936589974251\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.940294955327\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.944384458426\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94625311024591474]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.93279871692060945]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.93432178404641708]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.93440905231378846]\n",
      " AUC (Decision Tree)\n",
      " [0.92790815604509647]\n",
      " AUC (Random Forest)\n",
      " [0.93395708523986976]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.93675173357233832]\n",
      " AUC (Weighted Average)\n",
      " [0.94395431388273032]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94267005519128266]\n",
      " AUC (Stacking)\n",
      " [0.94429572149629692]\n",
      " AUC (Blending)\n",
      " [0.94463919288645704]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 6min 4s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('BACKWARD DIFFERENCE ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_backward_difference_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELMERT ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7085 - acc: 0.8784     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4880 - acc: 0.8959     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.0436 - acc: 0.8851     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.6028 - acc: 0.8935     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.6214 - acc: 0.8905     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4645 - acc: 0.9005     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.6397 - acc: 0.8581     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3889 - acc: 0.8691     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.8595 - acc: 0.7580     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3760 - acc: 0.8645     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.4921 - acc: 0.8625     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3344 - acc: 0.8704     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.1099 - acc: 0.8796     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.6988 - acc: 0.8752     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.0867 - acc: 0.8759     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.9654 - acc: 0.8816     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.0274 - acc: 0.8625     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.8071 - acc: 0.8807     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.9307 - acc: 0.7523     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3788 - acc: 0.8625     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 1.6984 - acc: 0.8313     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.5082 - acc: 0.8610     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 3.1555 - acc: 0.6273     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.6220 - acc: 0.8134     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4506 - acc: 0.8222     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2844 - acc: 0.8970     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4200 - acc: 0.8441     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2650 - acc: 0.9018     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4618 - acc: 0.8205     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2871 - acc: 0.9010     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3642 - acc: 0.8764     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2385 - acc: 0.9044     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2182 - acc: 0.9069     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2066 - acc: 0.9088     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2020 - acc: 0.9102     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1972 - acc: 0.9090     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1938 - acc: 0.9117     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1892 - acc: 0.9119     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1896 - acc: 0.9108     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1891 - acc: 0.9112     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1856 - acc: 0.9119     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1867 - acc: 0.9105     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1848 - acc: 0.9124     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1876 - acc: 0.9108     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1852 - acc: 0.9140     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.932698162937\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.933182972264\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.934635551162\n",
      " AUC (Decision Tree)\n",
      " 0.930391729664\n",
      " AUC (Random Forest)\n",
      " 0.935085548215\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.940394077854\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.946792707774\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94665158047432985]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.9327981272701541]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.93235883768102257]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.93439843860559468]\n",
      " AUC (Decision Tree)\n",
      " [0.92785390820321723]\n",
      " AUC (Random Forest)\n",
      " [0.9306691943016181]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.93939395726213493]\n",
      " AUC (Weighted Average)\n",
      " [0.94522442096325299]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94557791641115141]\n",
      " AUC (Stacking)\n",
      " [0.93980199537714038]\n",
      " AUC (Blending)\n",
      " [0.94677461201000046]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 7min 33s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('HELMERT ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_helmert_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.8911 - acc: 0.5734     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.6482 - acc: 0.7579     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 2.8561 - acc: 0.4978     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.8373 - acc: 0.7285     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 2.0848 - acc: 0.6023     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3071 - acc: 0.6688     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.9868 - acc: 0.6422     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.8251 - acc: 0.5704     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.9154 - acc: 0.6792     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7285 - acc: 0.6840     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7858 - acc: 0.6976     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.7423 - acc: 0.6091     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.6628 - acc: 0.3538     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.3364 - acc: 0.3047     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 6.4662 - acc: 0.3512     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.8992 - acc: 0.4256     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 2s - loss: 3.9074 - acc: 0.4460     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.4338 - acc: 0.3771     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3757 - acc: 0.8931     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2353 - acc: 0.9043     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3615 - acc: 0.8963     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2286 - acc: 0.9060     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4177 - acc: 0.8698     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2448 - acc: 0.9043     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 0s - loss: 3.5983 - acc: 0.5236     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 3.1725 - acc: 0.7090     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.4998 - acc: 0.6577     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.9612 - acc: 0.7048     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 1.2153 - acc: 0.5418     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4878 - acc: 0.8020     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3788 - acc: 0.8774     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2272 - acc: 0.9090     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2122 - acc: 0.9110     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2039 - acc: 0.9126     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1998 - acc: 0.9127     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1939 - acc: 0.9123     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1895 - acc: 0.9139     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1873 - acc: 0.9144     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1846 - acc: 0.9167     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1833 - acc: 0.9143     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1817 - acc: 0.9165     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1807 - acc: 0.9195     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1804 - acc: 0.9172     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1804 - acc: 0.9148     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1783 - acc: 0.9189     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.934606679251\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.935097064948\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.934988405809\n",
      " AUC (Decision Tree)\n",
      " 0.919397428219\n",
      " AUC (Random Forest)\n",
      " 0.936777605274\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.941016811344\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.945193410647\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94576681582801547]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.92927732440209443]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.92921128355111082]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.92874251143921882]\n",
      " AUC (Decision Tree)\n",
      " [0.92010855464880414]\n",
      " AUC (Random Forest)\n",
      " [0.93582421340629285]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.93814212934572383]\n",
      " AUC (Weighted Average)\n",
      " [0.94597563564319065]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94689725930468416]\n",
      " AUC (Stacking)\n",
      " [0.9445893674229916]\n",
      " AUC (Blending)\n",
      " [0.94644175432803423]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 8min 25s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SUM ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_sum_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL ENCODING\n",
      "\n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3442 - acc: 0.8886     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2216 - acc: 0.9068     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3351 - acc: 0.8867     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2156 - acc: 0.9084     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3611 - acc: 0.8824     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2237 - acc: 0.9056     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4999 - acc: 0.7833     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2821 - acc: 0.8994     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4449 - acc: 0.8445     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2689 - acc: 0.9009     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.5810 - acc: 0.6881     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 0s - loss: 0.2927 - acc: 0.8992     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3868 - acc: 0.8648     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2905 - acc: 0.8939     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.5111 - acc: 0.7539     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3026 - acc: 0.8903     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3763 - acc: 0.8614     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2825 - acc: 0.8943     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3298 - acc: 0.8910     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2210 - acc: 0.9060     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3878 - acc: 0.8633     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2260 - acc: 0.9076     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3333 - acc: 0.8921     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2198 - acc: 0.9062     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.4246 - acc: 0.8173     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2551 - acc: 0.9000     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3250 - acc: 0.8881     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2509 - acc: 0.9016     \n",
      "Epoch 1/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.3139 - acc: 0.8927     \n",
      "Epoch 2/2\n",
      "12356/12356 [==============================] - 1s - loss: 0.2543 - acc: 0.9011     \n",
      "Epoch 1/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.3082 - acc: 0.8912     \n",
      "Epoch 2/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2133 - acc: 0.9086     \n",
      "Epoch 3/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.2021 - acc: 0.9106     \n",
      "Epoch 4/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1944 - acc: 0.9125     \n",
      "Epoch 5/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1891 - acc: 0.9144     \n",
      "Epoch 6/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1840 - acc: 0.9146     \n",
      "Epoch 7/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1806 - acc: 0.9161     \n",
      "Epoch 8/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1780 - acc: 0.9184     \n",
      "Epoch 9/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1772 - acc: 0.9185     \n",
      "Epoch 10/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1757 - acc: 0.9183     \n",
      "Epoch 11/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1743 - acc: 0.9196     \n",
      "Epoch 12/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1728 - acc: 0.9217     \n",
      "Epoch 13/15\n",
      "18534/18534 [==============================] - 2s - loss: 0.1725 - acc: 0.9200     \n",
      "Epoch 14/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1696 - acc: 0.9201     \n",
      "Epoch 15/15\n",
      "18534/18534 [==============================] - 1s - loss: 0.1706 - acc: 0.9203     \n",
      "\n",
      "Start Cross Validation Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " 0.932706593011\n",
      " AUC (Logistic Regression - L2)\n",
      " 0.934614570615\n",
      " AUC (Logistic Regression - L1)\n",
      " 0.934692829073\n",
      " AUC (Decision Tree)\n",
      " 0.927573493355\n",
      " AUC (Random Forest)\n",
      " 0.933148713259\n",
      " AUC (Multi Layer Perceptron)\n",
      " 0.94208326665\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " 0.945101611655\n",
      "\n",
      "End Cross Validation Sample 0 \n",
      "\n",
      " AUC (Weighted Average)\n",
      " [0.94675801197716947]\n",
      "\n",
      "Start Test Sample 0 \n",
      "\n",
      " AUC (Linear Regression)\n",
      " [0.93286180951931685]\n",
      " AUC (Logistic Regression - L2)\n",
      " [0.93445740365111563]\n",
      " AUC (Logistic Regression - L1)\n",
      " [0.93452639275437532]\n",
      " AUC (Decision Tree)\n",
      " [0.92803935327138076]\n",
      " AUC (Random Forest)\n",
      " [0.92966944195480927]\n",
      " AUC (Multi Layer Perceptron)\n",
      " [0.93860913250625011]\n",
      " AUC (Weighted Average)\n",
      " [0.94443311005236097]\n",
      " AUC (Gradient Boosting - XGBoost)\n",
      " [0.94345930232558151]\n",
      " AUC (Stacking)\n",
      " [0.944130029718383]\n",
      " AUC (Blending)\n",
      " [0.94639635124298316]\n",
      "\n",
      "End Test Sample 0 \n",
      "\n",
      "1 loop, best of 1: 6min 19s per loop\n",
      "\n",
      "END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('POLYNOMIAL ENCODING\\n')\n",
    "%timeit -n1 -r1 sample_generation_polynomial_encode(1)\n",
    "print('\\nEND\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(Parallel(n_jobs=-1)(delayed(sample_generation)(n) for n in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
